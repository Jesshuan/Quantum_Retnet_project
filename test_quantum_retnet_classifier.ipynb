{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesshuan/miniconda3/envs/torch_quantum/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from quantum_retnet.modeling_quantum_retnet import RetNetForSequenceClassification\n",
    "from quantum_retnet.configuration_quantum_retnet import load_config_from_json\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosen backend : None and session : None\n",
      "Choosen backend : None and session : None\n"
     ]
    }
   ],
   "source": [
    "model = RetNetForSequenceClassification.from_pretrained(\"./model_store/model_small_quantum_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_stock = [\"It's great ! Wonderfull !\", \\\n",
    "                  \"It's very bad...\", \\\n",
    "                    \"I hate this film...\" \\\n",
    "                    \"I'm very hungry with this play... All actors has playing very bad...\",\n",
    "                    \"Sometimes, i'm really wonder if i don't spend my time with productions built from this production house\",\n",
    "                    \"I had a very good feeling to come in this place...\",\n",
    "                    \"All the protagonist are very realistic, the plot is captivating\",\n",
    "                    \"Don't care about this... be happy and that's all !\",\n",
    "                    \"Alice was very busy... She should take better care of herself\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- \n",
      " It's great ! Wonderfull !\n",
      "=> 1 -------------------------------------------- [tensor([-0.3325,  1.0280], grad_fn=<UnbindBackward0>)]\n",
      "--- \n",
      " It's very bad...\n",
      "=> 1 -------------------------------------------- [tensor([-0.3235,  1.0328], grad_fn=<UnbindBackward0>)]\n",
      "--- \n",
      " I hate this film...I'm very hungry with this play... All actors has playing very bad...\n",
      "=> 1 -------------------------------------------- [tensor([-0.3004,  1.0335], grad_fn=<UnbindBackward0>)]\n",
      "--- \n",
      " Sometimes, i'm really wonder if i don't spend my time with productions built from this production house\n",
      "=> 1 -------------------------------------------- [tensor([-0.3039,  1.0357], grad_fn=<UnbindBackward0>)]\n",
      "--- \n",
      " I had a very good feeling to come in this place...\n",
      "=> 1 -------------------------------------------- [tensor([-0.2610,  1.0437], grad_fn=<UnbindBackward0>)]\n",
      "--- \n",
      " All the protagonist are very realistic, the plot is captivating\n",
      "=> 1 -------------------------------------------- [tensor([-0.2553,  1.0485], grad_fn=<UnbindBackward0>)]\n",
      "--- \n",
      " Don't care about this... be happy and that's all !\n",
      "=> 1 -------------------------------------------- [tensor([-0.2563,  1.0428], grad_fn=<UnbindBackward0>)]\n",
      "--- \n",
      " Alice was very busy... She should take better care of herself\n",
      "=> 1 -------------------------------------------- [tensor([-0.2185,  1.0509], grad_fn=<UnbindBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentence_stock:\n",
    "    print(f\"--- \\n {sentence}\")\n",
    "    input = tokenizer(sentence,\n",
    "                      truncation=True,\n",
    "                    padding='max_length',\n",
    "                    max_length=48,\n",
    "                    return_tensors='pt')\n",
    "    class_predicted = model(input[\"input_ids\"]).logits\n",
    "    print(f\"=> {class_predicted.argmax()} -------------------------------------------- {list(class_predicted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1026, 318, 83, 1049, 5145, 1026, 318, 2089, 986]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(['It ist great !', \"It is bad...\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3603146892.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    context_inputs = tokenizer(, return_tensors='pt')\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "context_inputs = tokenizer(, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  40,  836,  470, 1833,  986]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutputWithPast(loss=None, logits=tensor([[ 0.4269, -0.0693]], grad_fn=<IndexBackward0>), past_key_values=({'prev_key_value': tensor([[[[-0.3554,  0.1702, -0.1555,  ...,  0.0744, -0.0582,  0.0994],\n",
       "          [-0.1459,  0.0344,  0.0225,  ...,  0.0431, -0.0960, -0.0581],\n",
       "          [ 0.1198,  0.0479,  0.0368,  ..., -0.0347,  0.3056,  0.0968],\n",
       "          ...,\n",
       "          [-0.2426,  0.1347,  0.0058,  ...,  0.0787,  0.0954,  0.0242],\n",
       "          [-0.0512, -0.0976, -0.0571,  ..., -0.1208,  0.0836,  0.0258],\n",
       "          [-0.0028, -0.0549,  0.0241,  ...,  0.0038, -0.0247, -0.0483]],\n",
       "\n",
       "         [[ 0.0044, -0.0332, -0.3063,  ...,  0.2979, -0.4021,  0.0009],\n",
       "          [ 0.0870, -0.1101, -0.4006,  ...,  0.2710, -0.3908, -0.0211],\n",
       "          [-0.1471,  0.0977,  0.0821,  ...,  0.1283, -0.0611, -0.1471],\n",
       "          ...,\n",
       "          [ 0.1809, -0.0902, -0.1389,  ...,  0.0034, -0.0636,  0.2156],\n",
       "          [ 0.2638, -0.1396, -0.0894,  ..., -0.0811,  0.0369,  0.1392],\n",
       "          [-0.1615,  0.1795,  0.2210,  ...,  0.2388, -0.2060, -0.1571]],\n",
       "\n",
       "         [[-0.0122,  0.0023, -0.0125,  ..., -0.0635, -0.0339, -0.0724],\n",
       "          [ 0.0123, -0.2011, -0.0660,  ...,  0.2506,  0.4216,  0.2938],\n",
       "          [ 0.1846,  0.0008,  0.0017,  ...,  0.0724,  0.0271,  0.1544],\n",
       "          ...,\n",
       "          [-0.0099, -0.1222, -0.0672,  ...,  0.2287,  0.2657,  0.1857],\n",
       "          [ 0.2416,  0.0622,  0.0106,  ..., -0.0068, -0.1057,  0.0836],\n",
       "          [-0.0867,  0.0754, -0.1243,  ...,  0.1659, -0.0949, -0.0927]],\n",
       "\n",
       "         [[ 0.1306, -0.1136, -0.0461,  ..., -0.1522,  0.0625,  0.2549],\n",
       "          [-0.0116, -0.0199,  0.0502,  ...,  0.0186,  0.0715,  0.0212],\n",
       "          [ 0.1477,  0.0632, -0.0375,  ..., -0.0466,  0.2744,  0.1038],\n",
       "          ...,\n",
       "          [ 0.1754, -0.1127,  0.1542,  ..., -0.1029, -0.0871,  0.1050],\n",
       "          [-0.0617,  0.1957,  0.0811,  ...,  0.2018,  0.3119, -0.2393],\n",
       "          [-0.1609,  0.0849, -0.1489,  ..., -0.0223, -0.0583,  0.2101]]]],\n",
       "       grad_fn=<SumBackward1>), 'scale': tensor([[[[4.6971]],\n",
       "\n",
       "         [[4.8462]],\n",
       "\n",
       "         [[4.9225]],\n",
       "\n",
       "         [[4.9611]]]])}, {'prev_key_value': tensor([[[[-2.3461e+00,  5.5462e-01,  6.4570e-01,  ..., -4.1768e-01,\n",
       "            6.9520e-01,  1.6996e+00],\n",
       "          [ 3.3973e+00, -1.0658e+00, -7.8728e-01,  ..., -7.7622e-02,\n",
       "           -1.9425e-01, -1.7389e+00],\n",
       "          [-2.3524e-01,  2.0989e-01,  2.6402e-01,  ..., -6.7206e-01,\n",
       "            9.6127e-01,  1.1241e+00],\n",
       "          ...,\n",
       "          [-2.6540e+00,  9.5830e-01,  6.3920e-01,  ..., -2.7725e-01,\n",
       "            5.6616e-01,  1.7263e+00],\n",
       "          [ 3.0534e+00, -2.4347e-01, -8.0052e-01,  ...,  3.9145e-01,\n",
       "           -5.6142e-01, -1.7340e+00],\n",
       "          [-1.3554e+00,  5.1684e-03,  5.2112e-01,  ..., -7.0513e-01,\n",
       "            9.4968e-01,  1.4754e+00]],\n",
       "\n",
       "         [[-1.1955e+00,  4.3088e-01,  2.0378e+00,  ..., -7.9852e+00,\n",
       "            6.6063e+00,  9.1967e+00],\n",
       "          [ 7.4659e-02,  1.7544e-01, -7.4247e-01,  ..., -2.0854e-01,\n",
       "            1.8392e-01, -1.0963e-01],\n",
       "          [-7.2464e-01,  9.8918e-02,  1.3132e+00,  ..., -5.5649e+00,\n",
       "            4.5810e+00,  6.5490e+00],\n",
       "          ...,\n",
       "          [ 7.5143e-01, -3.4006e-01, -1.0432e+00,  ...,  5.2894e+00,\n",
       "           -4.3781e+00, -5.9620e+00],\n",
       "          [-1.0381e+00,  3.7177e-01,  1.7330e+00,  ..., -6.5949e+00,\n",
       "            5.4509e+00,  7.5353e+00],\n",
       "          [-6.7856e-02,  5.3240e-01, -8.2455e-01,  ..., -2.1333e-01,\n",
       "            2.2070e-01, -4.0866e-01]],\n",
       "\n",
       "         [[ 7.6698e-02, -7.6474e-01,  1.1167e-01,  ..., -7.0005e-01,\n",
       "            8.9119e-01, -3.2012e-01],\n",
       "          [-3.0146e-01,  2.7707e-01, -7.2301e-02,  ...,  9.8866e-02,\n",
       "           -5.8509e-01,  4.7163e-02],\n",
       "          [ 6.6944e-01,  4.5553e+00, -7.3500e-01,  ..., -3.6222e+00,\n",
       "            1.8919e+00, -3.1198e+00],\n",
       "          ...,\n",
       "          [-1.5598e-01, -5.5947e-01,  2.7166e-01,  ...,  5.7350e-02,\n",
       "            7.7651e-02,  1.3721e-01],\n",
       "          [-1.4844e+00, -5.3054e+00,  6.7278e-01,  ...,  1.4342e+00,\n",
       "           -7.3446e-01,  1.8975e+00],\n",
       "          [ 5.5778e-02,  8.1261e-01, -3.9688e-01,  ..., -2.2441e-02,\n",
       "           -3.4287e-01, -1.4884e-01]],\n",
       "\n",
       "         [[-1.3829e+00, -1.0404e+00, -3.9386e-01,  ...,  7.1467e-01,\n",
       "           -1.7062e+00, -1.7729e+00],\n",
       "          [-1.0058e+00, -7.3586e-01, -3.1468e-01,  ...,  3.9807e-01,\n",
       "           -9.4206e-01, -9.9292e-01],\n",
       "          [ 4.0658e-01,  1.0353e+00, -1.0367e+00,  ..., -1.7274e-01,\n",
       "            3.9838e-01,  4.3839e-01],\n",
       "          ...,\n",
       "          [ 1.0873e+00, -2.4458e-02,  1.0785e+00,  ..., -5.1462e-01,\n",
       "            1.3567e+00,  1.3834e+00],\n",
       "          [ 3.2496e-01, -6.2032e-01,  1.6989e+00,  ..., -1.2279e+00,\n",
       "            2.9452e+00,  2.9290e+00],\n",
       "          [-1.3816e+00, -7.9444e-01, -7.7699e-01,  ...,  9.2457e-01,\n",
       "           -2.2486e+00, -2.2943e+00]]]], grad_fn=<SumBackward1>), 'scale': tensor([[[[4.6971]],\n",
       "\n",
       "         [[4.8462]],\n",
       "\n",
       "         [[4.9225]],\n",
       "\n",
       "         [[4.9611]]]])}, {'prev_key_value': tensor([[[[-5.3201e-02, -1.2209e-02, -5.8290e-02,  ..., -1.0004e+00,\n",
       "            6.3066e-01, -6.5167e-01],\n",
       "          [ 4.7399e-02,  3.5507e-02,  7.9101e-02,  ...,  3.6897e-02,\n",
       "           -3.0069e-02,  2.6110e-01],\n",
       "          [-9.0486e-01,  6.5625e-04, -8.6231e-01,  ..., -5.7300e+00,\n",
       "            4.1362e+00, -6.8448e+00],\n",
       "          ...,\n",
       "          [ 2.5629e-02, -1.0479e-02,  1.7206e-02,  ..., -5.7937e-04,\n",
       "           -4.3838e-02,  3.0882e-01],\n",
       "          [-2.5111e-04, -1.5091e-02, -1.8561e-02,  ..., -1.6242e-02,\n",
       "            1.9434e-04, -3.9858e-03],\n",
       "          [-1.7107e-02,  2.0145e-02, -1.3526e-03,  ..., -3.9933e-02,\n",
       "            5.3711e-02, -8.1338e-02]],\n",
       "\n",
       "         [[-9.8305e-02,  2.6648e+00,  1.0734e+00,  ..., -3.8400e-01,\n",
       "            7.7218e-01,  7.4604e-01],\n",
       "          [-6.3732e-01, -1.6286e-01, -1.0916e-01,  ...,  1.3491e+00,\n",
       "           -2.1523e+00, -2.3375e+00],\n",
       "          [-1.0978e+00, -2.6081e+00, -1.6891e-01,  ...,  3.7105e+00,\n",
       "           -5.8877e+00, -6.4469e+00],\n",
       "          ...,\n",
       "          [ 2.3008e-01, -7.7130e-01,  9.1107e-02,  ..., -2.3277e-01,\n",
       "            3.7533e-01,  3.8167e-01],\n",
       "          [ 1.3526e-01, -1.9337e+00,  2.8443e-01,  ...,  2.0387e-01,\n",
       "           -3.0496e-01, -4.1136e-01],\n",
       "          [ 2.7670e-01,  2.3192e+00,  5.4877e-02,  ..., -2.1590e+00,\n",
       "            3.4051e+00,  3.7424e+00]],\n",
       "\n",
       "         [[-1.0709e+00,  1.2258e+00, -1.1031e+00,  ...,  8.3509e-01,\n",
       "            3.9079e-01,  3.2686e+00],\n",
       "          [ 9.5406e-01, -4.8420e-01,  1.0781e+00,  ..., -7.2340e-01,\n",
       "           -1.3823e-01, -2.3394e+00],\n",
       "          [-5.5180e-01,  5.8896e-01, -6.5441e-01,  ...,  6.5161e-01,\n",
       "            2.7238e-01,  2.3598e+00],\n",
       "          ...,\n",
       "          [-1.6638e-01,  6.4238e-02, -2.0135e-01,  ...,  1.4048e-01,\n",
       "           -3.9827e-02,  2.9682e-01],\n",
       "          [-1.8113e-01,  2.9534e-01, -1.2032e-01,  ..., -2.3232e-02,\n",
       "           -2.2257e-02, -2.6338e-02],\n",
       "          [ 4.3148e-01, -3.5452e-01,  4.4328e-01,  ..., -3.0765e-01,\n",
       "           -1.7001e-02, -9.4010e-01]],\n",
       "\n",
       "         [[ 2.6791e-01,  8.3664e-01,  1.4224e+00,  ...,  8.1908e-01,\n",
       "            3.8978e-01,  7.0412e-02],\n",
       "          [ 1.5855e-02,  2.0005e-02, -1.6846e-02,  ..., -3.3691e-02,\n",
       "           -1.9160e-03, -3.9803e-02],\n",
       "          [ 5.0744e-02,  7.0402e-02,  1.7750e-01,  ..., -2.3859e-01,\n",
       "            6.6891e-03, -1.5058e-01],\n",
       "          ...,\n",
       "          [ 6.4420e-01, -1.3727e-03,  1.6254e+00,  ..., -5.9069e+00,\n",
       "           -4.3606e-01, -2.9011e+00],\n",
       "          [ 3.4280e-02,  1.0513e-02,  2.8127e-02,  ..., -3.4053e-02,\n",
       "            4.3318e-03, -2.4005e-02],\n",
       "          [-8.5684e-02, -1.7410e-01, -7.8755e-02,  ...,  4.8273e-02,\n",
       "           -4.1006e-02,  1.5295e-01]]]], grad_fn=<SumBackward1>), 'scale': tensor([[[[4.6971]],\n",
       "\n",
       "         [[4.8462]],\n",
       "\n",
       "         [[4.9225]],\n",
       "\n",
       "         [[4.9611]]]])}, {'prev_key_value': tensor([[[[-9.8657e-03, -8.4393e-04, -3.1287e-02,  ...,  5.0454e-02,\n",
       "            2.2705e-02, -1.8975e-02],\n",
       "          [-1.1381e-01, -3.9692e-02, -3.7267e-01,  ...,  1.3370e+00,\n",
       "            6.5253e-01, -4.5500e-01],\n",
       "          [ 6.4657e-02,  2.1230e-02,  2.0251e-01,  ..., -9.0736e-01,\n",
       "           -4.4503e-01,  2.9728e-01],\n",
       "          ...,\n",
       "          [ 5.3764e-02,  2.1227e-02,  2.4529e-01,  ..., -1.0525e+00,\n",
       "           -5.1094e-01,  3.3473e-01],\n",
       "          [-3.6812e-03, -5.7508e-04,  1.1883e-02,  ..., -5.4394e-03,\n",
       "           -1.4629e-03,  7.3646e-04],\n",
       "          [-3.3326e-03, -8.9854e-04, -7.3703e-03,  ...,  9.4704e-03,\n",
       "            2.9352e-03, -3.6798e-03]],\n",
       "\n",
       "         [[ 5.1441e-02, -1.5892e-01,  1.2831e-01,  ...,  1.2789e+00,\n",
       "           -7.6839e-01, -4.7461e-01],\n",
       "          [-1.2752e-02,  2.2175e-02, -3.9398e-03,  ..., -1.1679e-01,\n",
       "            7.2307e-02,  4.3507e-02],\n",
       "          [ 2.8556e-02, -7.0830e-02,  4.0115e-03,  ...,  9.9423e-02,\n",
       "           -6.1530e-02, -4.4900e-02],\n",
       "          ...,\n",
       "          [ 1.6696e-02, -4.2943e-02,  1.7959e-02,  ...,  1.1387e-01,\n",
       "           -8.0434e-02, -3.3843e-02],\n",
       "          [-8.9270e-02,  1.5683e-01, -2.3728e-02,  ...,  2.6385e+00,\n",
       "           -1.3070e+00, -1.1966e+00],\n",
       "          [-5.1441e-03,  8.7031e-03,  5.2307e-03,  ..., -1.1729e-02,\n",
       "            9.1918e-03,  3.1336e-03]],\n",
       "\n",
       "         [[ 2.6232e-01,  2.6658e-01,  1.5967e-01,  ...,  2.9621e-01,\n",
       "           -2.7133e-01, -7.5411e-02],\n",
       "          [-1.3450e-01, -1.5444e-01, -1.9910e-01,  ..., -7.6920e-01,\n",
       "            6.5400e-01,  1.4102e-01],\n",
       "          [-1.3398e-01,  1.3533e-01, -1.6158e-01,  ..., -1.0257e+00,\n",
       "            8.3952e-01,  1.6951e-01],\n",
       "          ...,\n",
       "          [ 3.3694e-01,  6.3481e-02,  1.0375e-01,  ...,  6.8909e-01,\n",
       "           -5.6478e-01, -1.2686e-01],\n",
       "          [-1.3395e-01,  1.4432e-01, -1.1420e+00,  ..., -6.0288e+00,\n",
       "            5.0148e+00,  1.0018e+00],\n",
       "          [ 1.4655e-02,  2.9947e-02, -4.3338e-02,  ..., -3.4611e-01,\n",
       "            2.8234e-01,  5.3166e-02]],\n",
       "\n",
       "         [[-1.7301e-01,  5.6315e-01, -2.2313e-02,  ...,  2.6762e-01,\n",
       "           -3.3341e-01, -1.0558e-01],\n",
       "          [-5.6986e-02,  2.7147e-01,  1.7754e-02,  ...,  7.2297e-01,\n",
       "           -1.0336e+00,  5.9370e-01],\n",
       "          [ 9.2406e-02, -4.1244e-01,  6.4040e-02,  ...,  1.0277e+00,\n",
       "           -1.5237e+00,  1.3046e+00],\n",
       "          ...,\n",
       "          [-2.4201e-02, -7.9042e-02, -6.5513e-02,  ..., -1.4063e+00,\n",
       "            2.0560e+00, -1.4876e+00],\n",
       "          [-1.6656e-02, -6.9547e-02,  3.4840e-03,  ...,  1.6313e-01,\n",
       "           -2.3478e-01,  1.7547e-01],\n",
       "          [-2.8510e-01,  1.1086e+00, -3.0104e-03,  ...,  1.8833e+00,\n",
       "           -2.6708e+00,  1.2465e+00]]]], grad_fn=<SumBackward1>), 'scale': tensor([[[[4.6971]],\n",
       "\n",
       "         [[4.8462]],\n",
       "\n",
       "         [[4.9225]],\n",
       "\n",
       "         [[4.9611]]]])}), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(context_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel forward\n",
    "# our custom generate function\n",
    "generated = model.custom_generate(context_inputs['input_ids'], parallel_compute_prompt=True, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = model.generate(**context_inputs, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It was very interesting but ick ernest ichi ichi the killer ivan character accepts the news of his illness']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "context_sentence = [\"The best trip I've ever made is\",\\\n",
    "                    \"This film is very disappointing because\",\\\n",
    "                    \"Do you want we go to theater this nigth ? I'm very impatient to\",\\\n",
    "                     \"How do you feel about things in general ?\",\\\n",
    "                    \"Explain me what you're talking about...\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Context --- \n",
      " The best trip I've ever made is\n",
      "---- Response ---- \n",
      " [\"The best trip I've ever made is going to be something really good ichi the killer. ivan and bale reduced mainly to batting\"]\n",
      "----\n",
      "--- Context --- \n",
      " This film is very disappointing because\n",
      "---- Response ---- \n",
      " ['This film is very disappointing because of its many excesses. ian holm ian holm as the aged napoleon ']\n",
      "----\n",
      "--- Context --- \n",
      " Do you want we go to theater this nigth ? I'm very impatient to\n",
      "---- Response ---- \n",
      " [\"Do you want we go to theater this nigth? I'm very impatient to be fondly remembered in the endlessly challenging maze of moviegoing. ian holm as the mother\"]\n",
      "----\n",
      "--- Context --- \n",
      " How do you feel about things in general ?\n",
      "---- Response ---- \n",
      " ['How do you feel about things in general? ivan ivan is a prince of a fellow iced with this one to kill a the world']\n",
      "----\n",
      "--- Context --- \n",
      " Explain me what you're talking about...\n",
      "---- Response ---- \n",
      " [\"Explain me what you're talking about... a movie that, like shiner's organizing of the big fight, pulls off enough icky\"]\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for sentence in context_sentence:\n",
    "    print(f\"--- Context --- \\n {sentence}\")\n",
    "    context_inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    generated = model.custom_generate(context_inputs['input_ids'], parallel_compute_prompt=True, max_new_tokens=20)\n",
    "    print(f\"---- Response ---- \\n {tokenizer.batch_decode(generated)}\")\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retnet_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
