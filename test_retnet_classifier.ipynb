{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from retnet.modeling_retnet import RetNetForSequenceClassification\n",
    "from retnet.configuration_retnet import load_config_from_json\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RetNetForSequenceClassification.from_pretrained(\"./model_store/model_small_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def transform(sentence):\n",
    "        return re.sub('[^A-Za-z0-9.,;!?]+', ' ', sentence) + tokenizer.eos_token\n",
    "\n",
    "def tokenize_sentence(example):\n",
    "        example = transform(example)\n",
    "        input_ids = tokenizer(example,\n",
    "                              truncation=True,\n",
    "                              padding='max_length',\n",
    "                              max_length=48,\n",
    "                              return_tensors='pt')\n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_stock = [\"It's great ! Wonderfull !\", \\\n",
    "                  \"It's very bad...\", \\\n",
    "                    \"I hate this film...\" \\\n",
    "                    \"I'm very hungry with this play... All actors has playing very bad...\",\n",
    "                    \"Sometimes, i'm really wonder if i don't spend my time with productions built from this production house\",\n",
    "                    \"I had a very good feeling to come in this place...\",\n",
    "                    \"All the protagonist are very realistic, the plot is captivating\",\n",
    "                    \"Don't care about this... be happy and that's all !\",\n",
    "                    \"Alice was very busy... She should take better care of herself\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- \n",
      " It's great ! Wonderfull !\n",
      "{'input_ids': tensor([[ 1026,   264,  1049,  5145, 12902, 12853,  5145, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "=> 1 -------------------------------------------- [tensor([-0.9782,  1.2029], grad_fn=<UnbindBackward0>)]\n",
      "--- \n",
      " It's very bad...\n",
      "{'input_ids': tensor([[ 1026,   264,   845,  2089,   986, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "=> 0 -------------------------------------------- [tensor([ 0.6060, -2.1469], grad_fn=<UnbindBackward0>)]\n",
      "--- \n",
      " I hate this film...I'm very hungry with this play... All actors has playing very bad...\n",
      "{'input_ids': tensor([[   40,  5465,   428,  2646,   986,    40,   285,   845, 14720,   351,\n",
      "           428,   711,   986,  1439, 10544,   468,  2712,   845,  2089,   986,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "=> 0 -------------------------------------------- [tensor([ 0.2199, -1.5907], grad_fn=<UnbindBackward0>)]\n",
      "--- \n",
      " Sometimes, i'm really wonder if i don't spend my time with productions built from this production house\n",
      "{'input_ids': tensor([[15468,    11,  1312,   285,  1107,  4240,   611,  1312,   836,   256,\n",
      "          4341,   616,   640,   351, 32260,  3170,   422,   428,  3227,  2156,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "=> 0 -------------------------------------------- [tensor([-0.4787, -0.5200], grad_fn=<UnbindBackward0>)]\n",
      "--- \n",
      " I had a very good feeling to come in this place...\n",
      "{'input_ids': tensor([[   40,   550,   257,   845,   922,  4203,   284,  1282,   287,   428,\n",
      "          1295,   986, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "=> 1 -------------------------------------------- [tensor([-0.7509, -0.0396], grad_fn=<UnbindBackward0>)]\n",
      "--- \n",
      " All the protagonist are very realistic, the plot is captivating\n",
      "{'input_ids': tensor([[ 3237,   262, 20281,   389,   845, 12653,    11,   262,  7110,   318,\n",
      "          3144, 39438, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "=> 1 -------------------------------------------- [tensor([-0.5211,  0.2862], grad_fn=<UnbindBackward0>)]\n",
      "--- \n",
      " Don't care about this... be happy and that's all !\n",
      "{'input_ids': tensor([[ 3987,   256,  1337,   546,   428,   986,   307,  3772,   290,   326,\n",
      "           264,   477,  5145, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "=> 0 -------------------------------------------- [tensor([ 0.3880, -1.4937], grad_fn=<UnbindBackward0>)]\n",
      "--- \n",
      " Alice was very busy... She should take better care of herself\n",
      "{'input_ids': tensor([[44484,   373,   845,  8179,   986,  1375,   815,  1011,  1365,  1337,\n",
      "           286,  5223, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "=> 0 -------------------------------------------- [tensor([ 0.5809, -1.8351], grad_fn=<UnbindBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentence_stock:\n",
    "    print(f\"--- \\n {sentence}\")\n",
    "    input = tokenize_sentence(sentence)\n",
    "    print(input)\n",
    "    class_predicted = model(input[\"input_ids\"], input[\"attention_mask\"]).logits\n",
    "    print(f\"=> {class_predicted.argmax()} -------------------------------------------- {list(class_predicted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1026, 318, 83, 1049, 5145, 1026, 318, 2089, 986]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(['It ist great !', \"It is bad...\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/retnet_experiment/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:748\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 748\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[1;32m    750\u001b[0m     \u001b[39m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     \u001b[39m# # at-least2d\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[39m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    753\u001b[0m     \u001b[39m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    754\u001b[0m     \u001b[39m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    755\u001b[0m     \u001b[39m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_experiment/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:720\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39marray(value))\n\u001b[0;32m--> 720\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mtensor(value)\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 5 at dim 1 (got 6)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/test_retnet_classifier.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/test_retnet_classifier.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m context_inputs \u001b[39m=\u001b[39m tokenizer([\u001b[39m\"\u001b[39;49m\u001b[39mI don\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mt understand...\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mSorry, but I hate this\u001b[39;49m\u001b[39m\"\u001b[39;49m], return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_experiment/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2829\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2828\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2829\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2830\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2831\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_experiment/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2915\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2910\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2911\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2912\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2913\u001b[0m         )\n\u001b[1;32m   2914\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[0;32m-> 2915\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[1;32m   2916\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2917\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2918\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2919\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2920\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2921\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2922\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2923\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2924\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2925\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2926\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2927\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2928\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2929\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2930\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2931\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2932\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2933\u001b[0m     )\n\u001b[1;32m   2934\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2935\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[1;32m   2936\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   2937\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2953\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2954\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_experiment/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3106\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3096\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3097\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3098\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   3099\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3103\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3104\u001b[0m )\n\u001b[0;32m-> 3106\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m   3107\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   3108\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   3109\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   3110\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   3111\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   3112\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   3113\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   3114\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   3115\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   3116\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   3117\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   3118\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   3119\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   3120\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   3121\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   3122\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   3123\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3124\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_experiment/lib/python3.11/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:162\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m is_split_into_words \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mis_split_into_words\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    157\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_prefix_space \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    158\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou need to instantiate \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m with add_prefix_space=True \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mto use it with pretokenized inputs.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m )\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_batch_encode_plus(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_experiment/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:552\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mfor\u001b[39;00m input_ids \u001b[39min\u001b[39;00m sanitized_tokens[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    551\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[0;32m--> 552\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(sanitized_tokens, sanitized_encodings, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_experiment/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:223\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    219\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[1;32m    221\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[0;32m--> 223\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_experiment/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:764\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moverflowing_tokens\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    760\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    761\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    762\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    763\u001b[0m             ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    765\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    766\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpadding=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtruncation=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    767\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m features (`\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    768\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m expected).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    769\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "context_inputs = tokenizer(, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  40,  836,  470, 1833,  986]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutputWithPast(loss=None, logits=tensor([[ 0.4269, -0.0693]], grad_fn=<IndexBackward0>), past_key_values=({'prev_key_value': tensor([[[[-0.3554,  0.1702, -0.1555,  ...,  0.0744, -0.0582,  0.0994],\n",
       "          [-0.1459,  0.0344,  0.0225,  ...,  0.0431, -0.0960, -0.0581],\n",
       "          [ 0.1198,  0.0479,  0.0368,  ..., -0.0347,  0.3056,  0.0968],\n",
       "          ...,\n",
       "          [-0.2426,  0.1347,  0.0058,  ...,  0.0787,  0.0954,  0.0242],\n",
       "          [-0.0512, -0.0976, -0.0571,  ..., -0.1208,  0.0836,  0.0258],\n",
       "          [-0.0028, -0.0549,  0.0241,  ...,  0.0038, -0.0247, -0.0483]],\n",
       "\n",
       "         [[ 0.0044, -0.0332, -0.3063,  ...,  0.2979, -0.4021,  0.0009],\n",
       "          [ 0.0870, -0.1101, -0.4006,  ...,  0.2710, -0.3908, -0.0211],\n",
       "          [-0.1471,  0.0977,  0.0821,  ...,  0.1283, -0.0611, -0.1471],\n",
       "          ...,\n",
       "          [ 0.1809, -0.0902, -0.1389,  ...,  0.0034, -0.0636,  0.2156],\n",
       "          [ 0.2638, -0.1396, -0.0894,  ..., -0.0811,  0.0369,  0.1392],\n",
       "          [-0.1615,  0.1795,  0.2210,  ...,  0.2388, -0.2060, -0.1571]],\n",
       "\n",
       "         [[-0.0122,  0.0023, -0.0125,  ..., -0.0635, -0.0339, -0.0724],\n",
       "          [ 0.0123, -0.2011, -0.0660,  ...,  0.2506,  0.4216,  0.2938],\n",
       "          [ 0.1846,  0.0008,  0.0017,  ...,  0.0724,  0.0271,  0.1544],\n",
       "          ...,\n",
       "          [-0.0099, -0.1222, -0.0672,  ...,  0.2287,  0.2657,  0.1857],\n",
       "          [ 0.2416,  0.0622,  0.0106,  ..., -0.0068, -0.1057,  0.0836],\n",
       "          [-0.0867,  0.0754, -0.1243,  ...,  0.1659, -0.0949, -0.0927]],\n",
       "\n",
       "         [[ 0.1306, -0.1136, -0.0461,  ..., -0.1522,  0.0625,  0.2549],\n",
       "          [-0.0116, -0.0199,  0.0502,  ...,  0.0186,  0.0715,  0.0212],\n",
       "          [ 0.1477,  0.0632, -0.0375,  ..., -0.0466,  0.2744,  0.1038],\n",
       "          ...,\n",
       "          [ 0.1754, -0.1127,  0.1542,  ..., -0.1029, -0.0871,  0.1050],\n",
       "          [-0.0617,  0.1957,  0.0811,  ...,  0.2018,  0.3119, -0.2393],\n",
       "          [-0.1609,  0.0849, -0.1489,  ..., -0.0223, -0.0583,  0.2101]]]],\n",
       "       grad_fn=<SumBackward1>), 'scale': tensor([[[[4.6971]],\n",
       "\n",
       "         [[4.8462]],\n",
       "\n",
       "         [[4.9225]],\n",
       "\n",
       "         [[4.9611]]]])}, {'prev_key_value': tensor([[[[-2.3461e+00,  5.5462e-01,  6.4570e-01,  ..., -4.1768e-01,\n",
       "            6.9520e-01,  1.6996e+00],\n",
       "          [ 3.3973e+00, -1.0658e+00, -7.8728e-01,  ..., -7.7622e-02,\n",
       "           -1.9425e-01, -1.7389e+00],\n",
       "          [-2.3524e-01,  2.0989e-01,  2.6402e-01,  ..., -6.7206e-01,\n",
       "            9.6127e-01,  1.1241e+00],\n",
       "          ...,\n",
       "          [-2.6540e+00,  9.5830e-01,  6.3920e-01,  ..., -2.7725e-01,\n",
       "            5.6616e-01,  1.7263e+00],\n",
       "          [ 3.0534e+00, -2.4347e-01, -8.0052e-01,  ...,  3.9145e-01,\n",
       "           -5.6142e-01, -1.7340e+00],\n",
       "          [-1.3554e+00,  5.1684e-03,  5.2112e-01,  ..., -7.0513e-01,\n",
       "            9.4968e-01,  1.4754e+00]],\n",
       "\n",
       "         [[-1.1955e+00,  4.3088e-01,  2.0378e+00,  ..., -7.9852e+00,\n",
       "            6.6063e+00,  9.1967e+00],\n",
       "          [ 7.4659e-02,  1.7544e-01, -7.4247e-01,  ..., -2.0854e-01,\n",
       "            1.8392e-01, -1.0963e-01],\n",
       "          [-7.2464e-01,  9.8918e-02,  1.3132e+00,  ..., -5.5649e+00,\n",
       "            4.5810e+00,  6.5490e+00],\n",
       "          ...,\n",
       "          [ 7.5143e-01, -3.4006e-01, -1.0432e+00,  ...,  5.2894e+00,\n",
       "           -4.3781e+00, -5.9620e+00],\n",
       "          [-1.0381e+00,  3.7177e-01,  1.7330e+00,  ..., -6.5949e+00,\n",
       "            5.4509e+00,  7.5353e+00],\n",
       "          [-6.7856e-02,  5.3240e-01, -8.2455e-01,  ..., -2.1333e-01,\n",
       "            2.2070e-01, -4.0866e-01]],\n",
       "\n",
       "         [[ 7.6698e-02, -7.6474e-01,  1.1167e-01,  ..., -7.0005e-01,\n",
       "            8.9119e-01, -3.2012e-01],\n",
       "          [-3.0146e-01,  2.7707e-01, -7.2301e-02,  ...,  9.8866e-02,\n",
       "           -5.8509e-01,  4.7163e-02],\n",
       "          [ 6.6944e-01,  4.5553e+00, -7.3500e-01,  ..., -3.6222e+00,\n",
       "            1.8919e+00, -3.1198e+00],\n",
       "          ...,\n",
       "          [-1.5598e-01, -5.5947e-01,  2.7166e-01,  ...,  5.7350e-02,\n",
       "            7.7651e-02,  1.3721e-01],\n",
       "          [-1.4844e+00, -5.3054e+00,  6.7278e-01,  ...,  1.4342e+00,\n",
       "           -7.3446e-01,  1.8975e+00],\n",
       "          [ 5.5778e-02,  8.1261e-01, -3.9688e-01,  ..., -2.2441e-02,\n",
       "           -3.4287e-01, -1.4884e-01]],\n",
       "\n",
       "         [[-1.3829e+00, -1.0404e+00, -3.9386e-01,  ...,  7.1467e-01,\n",
       "           -1.7062e+00, -1.7729e+00],\n",
       "          [-1.0058e+00, -7.3586e-01, -3.1468e-01,  ...,  3.9807e-01,\n",
       "           -9.4206e-01, -9.9292e-01],\n",
       "          [ 4.0658e-01,  1.0353e+00, -1.0367e+00,  ..., -1.7274e-01,\n",
       "            3.9838e-01,  4.3839e-01],\n",
       "          ...,\n",
       "          [ 1.0873e+00, -2.4458e-02,  1.0785e+00,  ..., -5.1462e-01,\n",
       "            1.3567e+00,  1.3834e+00],\n",
       "          [ 3.2496e-01, -6.2032e-01,  1.6989e+00,  ..., -1.2279e+00,\n",
       "            2.9452e+00,  2.9290e+00],\n",
       "          [-1.3816e+00, -7.9444e-01, -7.7699e-01,  ...,  9.2457e-01,\n",
       "           -2.2486e+00, -2.2943e+00]]]], grad_fn=<SumBackward1>), 'scale': tensor([[[[4.6971]],\n",
       "\n",
       "         [[4.8462]],\n",
       "\n",
       "         [[4.9225]],\n",
       "\n",
       "         [[4.9611]]]])}, {'prev_key_value': tensor([[[[-5.3201e-02, -1.2209e-02, -5.8290e-02,  ..., -1.0004e+00,\n",
       "            6.3066e-01, -6.5167e-01],\n",
       "          [ 4.7399e-02,  3.5507e-02,  7.9101e-02,  ...,  3.6897e-02,\n",
       "           -3.0069e-02,  2.6110e-01],\n",
       "          [-9.0486e-01,  6.5625e-04, -8.6231e-01,  ..., -5.7300e+00,\n",
       "            4.1362e+00, -6.8448e+00],\n",
       "          ...,\n",
       "          [ 2.5629e-02, -1.0479e-02,  1.7206e-02,  ..., -5.7937e-04,\n",
       "           -4.3838e-02,  3.0882e-01],\n",
       "          [-2.5111e-04, -1.5091e-02, -1.8561e-02,  ..., -1.6242e-02,\n",
       "            1.9434e-04, -3.9858e-03],\n",
       "          [-1.7107e-02,  2.0145e-02, -1.3526e-03,  ..., -3.9933e-02,\n",
       "            5.3711e-02, -8.1338e-02]],\n",
       "\n",
       "         [[-9.8305e-02,  2.6648e+00,  1.0734e+00,  ..., -3.8400e-01,\n",
       "            7.7218e-01,  7.4604e-01],\n",
       "          [-6.3732e-01, -1.6286e-01, -1.0916e-01,  ...,  1.3491e+00,\n",
       "           -2.1523e+00, -2.3375e+00],\n",
       "          [-1.0978e+00, -2.6081e+00, -1.6891e-01,  ...,  3.7105e+00,\n",
       "           -5.8877e+00, -6.4469e+00],\n",
       "          ...,\n",
       "          [ 2.3008e-01, -7.7130e-01,  9.1107e-02,  ..., -2.3277e-01,\n",
       "            3.7533e-01,  3.8167e-01],\n",
       "          [ 1.3526e-01, -1.9337e+00,  2.8443e-01,  ...,  2.0387e-01,\n",
       "           -3.0496e-01, -4.1136e-01],\n",
       "          [ 2.7670e-01,  2.3192e+00,  5.4877e-02,  ..., -2.1590e+00,\n",
       "            3.4051e+00,  3.7424e+00]],\n",
       "\n",
       "         [[-1.0709e+00,  1.2258e+00, -1.1031e+00,  ...,  8.3509e-01,\n",
       "            3.9079e-01,  3.2686e+00],\n",
       "          [ 9.5406e-01, -4.8420e-01,  1.0781e+00,  ..., -7.2340e-01,\n",
       "           -1.3823e-01, -2.3394e+00],\n",
       "          [-5.5180e-01,  5.8896e-01, -6.5441e-01,  ...,  6.5161e-01,\n",
       "            2.7238e-01,  2.3598e+00],\n",
       "          ...,\n",
       "          [-1.6638e-01,  6.4238e-02, -2.0135e-01,  ...,  1.4048e-01,\n",
       "           -3.9827e-02,  2.9682e-01],\n",
       "          [-1.8113e-01,  2.9534e-01, -1.2032e-01,  ..., -2.3232e-02,\n",
       "           -2.2257e-02, -2.6338e-02],\n",
       "          [ 4.3148e-01, -3.5452e-01,  4.4328e-01,  ..., -3.0765e-01,\n",
       "           -1.7001e-02, -9.4010e-01]],\n",
       "\n",
       "         [[ 2.6791e-01,  8.3664e-01,  1.4224e+00,  ...,  8.1908e-01,\n",
       "            3.8978e-01,  7.0412e-02],\n",
       "          [ 1.5855e-02,  2.0005e-02, -1.6846e-02,  ..., -3.3691e-02,\n",
       "           -1.9160e-03, -3.9803e-02],\n",
       "          [ 5.0744e-02,  7.0402e-02,  1.7750e-01,  ..., -2.3859e-01,\n",
       "            6.6891e-03, -1.5058e-01],\n",
       "          ...,\n",
       "          [ 6.4420e-01, -1.3727e-03,  1.6254e+00,  ..., -5.9069e+00,\n",
       "           -4.3606e-01, -2.9011e+00],\n",
       "          [ 3.4280e-02,  1.0513e-02,  2.8127e-02,  ..., -3.4053e-02,\n",
       "            4.3318e-03, -2.4005e-02],\n",
       "          [-8.5684e-02, -1.7410e-01, -7.8755e-02,  ...,  4.8273e-02,\n",
       "           -4.1006e-02,  1.5295e-01]]]], grad_fn=<SumBackward1>), 'scale': tensor([[[[4.6971]],\n",
       "\n",
       "         [[4.8462]],\n",
       "\n",
       "         [[4.9225]],\n",
       "\n",
       "         [[4.9611]]]])}, {'prev_key_value': tensor([[[[-9.8657e-03, -8.4393e-04, -3.1287e-02,  ...,  5.0454e-02,\n",
       "            2.2705e-02, -1.8975e-02],\n",
       "          [-1.1381e-01, -3.9692e-02, -3.7267e-01,  ...,  1.3370e+00,\n",
       "            6.5253e-01, -4.5500e-01],\n",
       "          [ 6.4657e-02,  2.1230e-02,  2.0251e-01,  ..., -9.0736e-01,\n",
       "           -4.4503e-01,  2.9728e-01],\n",
       "          ...,\n",
       "          [ 5.3764e-02,  2.1227e-02,  2.4529e-01,  ..., -1.0525e+00,\n",
       "           -5.1094e-01,  3.3473e-01],\n",
       "          [-3.6812e-03, -5.7508e-04,  1.1883e-02,  ..., -5.4394e-03,\n",
       "           -1.4629e-03,  7.3646e-04],\n",
       "          [-3.3326e-03, -8.9854e-04, -7.3703e-03,  ...,  9.4704e-03,\n",
       "            2.9352e-03, -3.6798e-03]],\n",
       "\n",
       "         [[ 5.1441e-02, -1.5892e-01,  1.2831e-01,  ...,  1.2789e+00,\n",
       "           -7.6839e-01, -4.7461e-01],\n",
       "          [-1.2752e-02,  2.2175e-02, -3.9398e-03,  ..., -1.1679e-01,\n",
       "            7.2307e-02,  4.3507e-02],\n",
       "          [ 2.8556e-02, -7.0830e-02,  4.0115e-03,  ...,  9.9423e-02,\n",
       "           -6.1530e-02, -4.4900e-02],\n",
       "          ...,\n",
       "          [ 1.6696e-02, -4.2943e-02,  1.7959e-02,  ...,  1.1387e-01,\n",
       "           -8.0434e-02, -3.3843e-02],\n",
       "          [-8.9270e-02,  1.5683e-01, -2.3728e-02,  ...,  2.6385e+00,\n",
       "           -1.3070e+00, -1.1966e+00],\n",
       "          [-5.1441e-03,  8.7031e-03,  5.2307e-03,  ..., -1.1729e-02,\n",
       "            9.1918e-03,  3.1336e-03]],\n",
       "\n",
       "         [[ 2.6232e-01,  2.6658e-01,  1.5967e-01,  ...,  2.9621e-01,\n",
       "           -2.7133e-01, -7.5411e-02],\n",
       "          [-1.3450e-01, -1.5444e-01, -1.9910e-01,  ..., -7.6920e-01,\n",
       "            6.5400e-01,  1.4102e-01],\n",
       "          [-1.3398e-01,  1.3533e-01, -1.6158e-01,  ..., -1.0257e+00,\n",
       "            8.3952e-01,  1.6951e-01],\n",
       "          ...,\n",
       "          [ 3.3694e-01,  6.3481e-02,  1.0375e-01,  ...,  6.8909e-01,\n",
       "           -5.6478e-01, -1.2686e-01],\n",
       "          [-1.3395e-01,  1.4432e-01, -1.1420e+00,  ..., -6.0288e+00,\n",
       "            5.0148e+00,  1.0018e+00],\n",
       "          [ 1.4655e-02,  2.9947e-02, -4.3338e-02,  ..., -3.4611e-01,\n",
       "            2.8234e-01,  5.3166e-02]],\n",
       "\n",
       "         [[-1.7301e-01,  5.6315e-01, -2.2313e-02,  ...,  2.6762e-01,\n",
       "           -3.3341e-01, -1.0558e-01],\n",
       "          [-5.6986e-02,  2.7147e-01,  1.7754e-02,  ...,  7.2297e-01,\n",
       "           -1.0336e+00,  5.9370e-01],\n",
       "          [ 9.2406e-02, -4.1244e-01,  6.4040e-02,  ...,  1.0277e+00,\n",
       "           -1.5237e+00,  1.3046e+00],\n",
       "          ...,\n",
       "          [-2.4201e-02, -7.9042e-02, -6.5513e-02,  ..., -1.4063e+00,\n",
       "            2.0560e+00, -1.4876e+00],\n",
       "          [-1.6656e-02, -6.9547e-02,  3.4840e-03,  ...,  1.6313e-01,\n",
       "           -2.3478e-01,  1.7547e-01],\n",
       "          [-2.8510e-01,  1.1086e+00, -3.0104e-03,  ...,  1.8833e+00,\n",
       "           -2.6708e+00,  1.2465e+00]]]], grad_fn=<SumBackward1>), 'scale': tensor([[[[4.6971]],\n",
       "\n",
       "         [[4.8462]],\n",
       "\n",
       "         [[4.9225]],\n",
       "\n",
       "         [[4.9611]]]])}), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(context_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel forward\n",
    "# our custom generate function\n",
    "generated = model.custom_generate(context_inputs['input_ids'], parallel_compute_prompt=True, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = model.generate(**context_inputs, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It was very interesting but ick ernest ichi ichi the killer ivan character accepts the news of his illness']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "context_sentence = [\"The best trip I've ever made is\",\\\n",
    "                    \"This film is very disappointing because\",\\\n",
    "                    \"Do you want we go to theater this nigth ? I'm very impatient to\",\\\n",
    "                     \"How do you feel about things in general ?\",\\\n",
    "                    \"Explain me what you're talking about...\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Context --- \n",
      " The best trip I've ever made is\n",
      "---- Response ---- \n",
      " [\"The best trip I've ever made is going to be something really good ichi the killer. ivan and bale reduced mainly to batting\"]\n",
      "----\n",
      "--- Context --- \n",
      " This film is very disappointing because\n",
      "---- Response ---- \n",
      " ['This film is very disappointing because of its many excesses. ian holm ian holm as the aged napoleon ']\n",
      "----\n",
      "--- Context --- \n",
      " Do you want we go to theater this nigth ? I'm very impatient to\n",
      "---- Response ---- \n",
      " [\"Do you want we go to theater this nigth? I'm very impatient to be fondly remembered in the endlessly challenging maze of moviegoing. ian holm as the mother\"]\n",
      "----\n",
      "--- Context --- \n",
      " How do you feel about things in general ?\n",
      "---- Response ---- \n",
      " ['How do you feel about things in general? ivan ivan is a prince of a fellow iced with this one to kill a the world']\n",
      "----\n",
      "--- Context --- \n",
      " Explain me what you're talking about...\n",
      "---- Response ---- \n",
      " [\"Explain me what you're talking about... a movie that, like shiner's organizing of the big fight, pulls off enough icky\"]\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for sentence in context_sentence:\n",
    "    print(f\"--- Context --- \\n {sentence}\")\n",
    "    context_inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    generated = model.custom_generate(context_inputs['input_ids'], parallel_compute_prompt=True, max_new_tokens=20)\n",
    "    print(f\"---- Response ---- \\n {tokenizer.batch_decode(generated)}\")\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retnet_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
