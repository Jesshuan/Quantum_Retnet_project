{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesshuan/miniconda3/envs/torch_quantum/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qiskit_ibm_runtime import QiskitRuntimeService, Options, Sampler, Session, Estimator\n",
    "\n",
    "from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager\n",
    "\n",
    "from torchquantum.measurement import expval_joint_analytical, expval_joint_sampling, expval_joint_sampling_grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchquantum as tq\n",
    "\n",
    "import torchquantum.functional as tqf\n",
    "\n",
    "from torchquantum.plugin.qiskit import tq2qiskit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = QiskitRuntimeService(channel=\"ibm_quantum\", token=config[\"IBM_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<IBMBackend('simulator_mps')>,\n",
       " <IBMBackend('simulator_statevector')>,\n",
       " <IBMBackend('simulator_stabilizer')>,\n",
       " <IBMBackend('ibm_brisbane')>,\n",
       " <IBMBackend('ibm_kyoto')>,\n",
       " <IBMBackend('ibm_sherbrooke')>,\n",
       " <IBMBackend('ibmq_qasm_simulator')>,\n",
       " <IBMBackend('simulator_extended_stabilizer')>,\n",
       " <IBMBackend('ibm_osaka')>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service.backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = service.backend(\"ibmq_qasm_simulator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBase(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 mask=None,\n",
    "                 use_bias=False):\n",
    "        super(MultiHeadAttentionBase, self).__init__()\n",
    "\n",
    "        assert embed_dim % num_heads == 0, f\"Embedding dimension ({embed_dim}) should be divisible by number of heads ({num_heads})\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embed_dim // num_heads  # projection dimensions\n",
    "        self.k_linear = None\n",
    "        self.q_linear = None\n",
    "        self.v_linear = None\n",
    "        self.combine_heads = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn_weights = None\n",
    "    \n",
    "    def separate_heads(self, x):\n",
    "        '''\n",
    "        split into N heads\n",
    "        from (batch_size, seq_len, embed_dim)\n",
    "        to   (batch_size, seq_len, num_heads, embed_dim)\n",
    "        then transpose (1,2) to (batch_size, num_heads, seq_len, embed_dim)\n",
    "        to make mat mult straightforward for each head\n",
    "        '''\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    def attention(self, query, key, value, mask=None, dropout=None):\n",
    "        '''\n",
    "        Attention(Q, K, V) = softmax(Q K^T / sqrt(d_k))V\n",
    "        '''\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        # see also: https://tensorchiefs.github.io/dlday2018/tutorial/einsum.html\n",
    "        #scores = torch.einsum('bijh, bkjh -> bikh', query, key) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "        attn = torch.matmul(scores, value)\n",
    "        return attn, scores\n",
    "    \n",
    "    def downstream(self, query, key, value, batch_size, mask=None):\n",
    "        Q = self.separate_heads(query)\n",
    "        K = self.separate_heads(key)\n",
    "        V = self.separate_heads(value)\n",
    "\n",
    "        x, self.attn_weights = self.attention(Q, K, V, mask, dropout=self.dropout)\n",
    "\n",
    "        concat = x.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
    "\n",
    "        return concat\n",
    "        # output = self.combine_heads(concat)\n",
    "        # return output\n",
    "\n",
    "   # def forward(self, x, mask=None):\n",
    "    #    raise NotImplementedError(\"Base class does not execute forward function.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionClassical(MultiHeadAttentionBase):\n",
    "    \n",
    "    def __init__(self, embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 dropout=0.1,\n",
    "                 mask=None,\n",
    "                 use_bias=False):\n",
    "        super(MultiHeadAttentionClassical, self).__init__(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, mask=mask, use_bias=use_bias)\n",
    "\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
    "        self.combine_heads = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, _ , embed_dim = x.size()\n",
    "        assert embed_dim == self.embed_dim, f\"Input embedding ({embed_dim}) does not match layer embedding size ({self.embed_dim})\"\n",
    "\n",
    "        K = self.k_linear(x)\n",
    "        Q = self.q_linear(x)\n",
    "        V = self.v_linear(x)\n",
    "\n",
    "        x = self.downstream(Q, K, V, batch_size, mask)\n",
    "        output = self.combine_heads(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLayer(tq.QuantumModule):\n",
    "        def __init__(self, n_qbits, D_ansatz=1, *args, **kwargs):\n",
    "            super().__init__()    \n",
    "            self.n_wires = n_qbits\n",
    "            self.encoder = tq.GeneralEncoder(\n",
    "                    [{'input_idx': [i], 'func': 'rx', 'wires': [i]} for i in range(self.n_wires)])\n",
    "            #self.rx_list = [tq.RX(has_params=True, trainable=True) for _ in range(self.n_wires)]\n",
    "            #self.ry_test = tq.RY(has_params=True, trainable=True)\n",
    "            #self.measure = tq.MeasureAll(tq.PauliZ)\n",
    "\n",
    "            if n_qbits >= 2:\n",
    "                self.rx_0 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_1 = tq.RX(has_params=True, trainable=True)\n",
    "                self.ry_1_0 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_1 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_0 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_1 = tq.RY(has_params=True, trainable=True)\n",
    "            if n_qbits >= 4:\n",
    "                self.rx_2 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_3 = tq.RX(has_params=True, trainable=True)\n",
    "                self.ry_1_2 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_3 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_2 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_3 = tq.RY(has_params=True, trainable=True)\n",
    "            if n_qbits >= 8:\n",
    "                self.rx_4 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_5 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_6 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_7 = tq.RX(has_params=True, trainable=True)\n",
    "                self.ry_1_4 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_5 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_6 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_7 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_4 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_5 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_6 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_7 = tq.RY(has_params=True, trainable=True)\n",
    "            if n_qbits >= 16:\n",
    "                self.rx_8 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_9 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_10 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_11 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_12 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_13 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_14 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_15 = tq.RX(has_params=True, trainable=True)\n",
    "                self.ry_1_8 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_9 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_10 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_11 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_12 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_13 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_14 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_15 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_8 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_9 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_10 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_11 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_12 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_13 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_14 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_15 = tq.RY(has_params=True, trainable=True)\n",
    "            if n_qbits >= 32:\n",
    "                self.rx_16 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_17 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_18 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_19 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_20 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_21 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_22 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_23 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_24 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_25 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_26 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_27 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_28 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_29 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_30 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_31 = tq.RX(has_params=True, trainable=True)\n",
    "                self.ry_1_16 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_17 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_18 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_19 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_20 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_21 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_22 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_23 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_24 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_25 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_26 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_27 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_28 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_29 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_30 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_1_31 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_16 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_17 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_18 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_19 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_20 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_21 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_22 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_23 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_24 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_25 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_26 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_27 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_28 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_29 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_30 = tq.RY(has_params=True, trainable=True)\n",
    "                self.ry_2_31 = tq.RY(has_params=True, trainable=True)\n",
    "\n",
    "            #self.observables = SparsePauliOp.from_list([(\"ZX\" + \"I\"*(n_qbits - 2),1), (\"XY\" + \"I\"*(n_qbits - 2),1)] + [(\"I\"*i + \"Z\" + \"I\"*(n_qbits - 1 -i), 1) for i in range(n_qbits)])\n",
    "            #self.measure = tq.MeasureAll(tq.PauliZ)\n",
    "            #self.measure = tq.MeasureMultiPauliSum(self.observables)\n",
    "\n",
    "        def ansatz_gate_forward_rx(self, q_device):\n",
    "            if self.n_wires >= 2:\n",
    "                self.rx_0(q_device, wires=0)\n",
    "                self.rx_1(q_device, wires=1)\n",
    "            if self.n_wires >= 4:\n",
    "                self.rx_2(q_device, wires=2)\n",
    "                self.rx_3(q_device, wires=3)\n",
    "            if self.n_wires >= 8:\n",
    "                self.rx_4(q_device, wires=4)\n",
    "                self.rx_5(q_device, wires=5)\n",
    "                self.rx_6(q_device, wires=6)\n",
    "                self.rx_7(q_device, wires=7)\n",
    "            if self.n_wires >= 16:\n",
    "                self.rx_8(q_device, wires=8)\n",
    "                self.rx_9(q_device, wires=9)\n",
    "                self.rx_10(q_device, wires=10)\n",
    "                self.rx_11(q_device, wires=11)\n",
    "                self.rx_12(q_device, wires=12)\n",
    "                self.rx_13(q_device, wires=13)\n",
    "                self.rx_14(q_device, wires=14)\n",
    "                self.rx_15(q_device, wires=15)\n",
    "            if self.n_wires >= 32:\n",
    "                self.rx_16(q_device, wires=16)\n",
    "                self.rx_17(q_device, wires=17)\n",
    "                self.rx_18(q_device, wires=18)\n",
    "                self.rx_19(q_device, wires=19)\n",
    "                self.rx_20(q_device, wires=20)\n",
    "                self.rx_21(q_device, wires=21)\n",
    "                self.rx_22(q_device, wires=22)\n",
    "                self.rx_23(q_device, wires=23)\n",
    "                self.rx_24(q_device, wires=24)\n",
    "                self.rx_25(q_device, wires=25)\n",
    "                self.rx_26(q_device, wires=26)\n",
    "                self.rx_27(q_device, wires=27)\n",
    "                self.rx_28(q_device, wires=26)\n",
    "                self.rx_29(q_device, wires=29)\n",
    "                self.rx_30(q_device, wires=30)\n",
    "                self.rx_31(q_device, wires=31)\n",
    "            \n",
    "        def ansatz_gate_forward_ry_1(self, q_device):\n",
    "            if self.n_wires >= 2:\n",
    "                self.ry_1_0(q_device, wires=0)\n",
    "                self.ry_1_1(q_device, wires=1)\n",
    "            if self.n_wires >= 4:\n",
    "                self.ry_1_2(q_device, wires=2)\n",
    "                self.ry_1_3(q_device, wires=3)\n",
    "            if self.n_wires >= 8:\n",
    "                self.ry_1_4(q_device, wires=4)\n",
    "                self.ry_1_5(q_device, wires=5)\n",
    "                self.ry_1_6(q_device, wires=6)\n",
    "                self.ry_1_7(q_device, wires=7)\n",
    "            if self.n_wires >= 16:\n",
    "                self.ry_1_8(q_device, wires=8)\n",
    "                self.ry_1_9(q_device, wires=9)\n",
    "                self.ry_1_10(q_device, wires=10)\n",
    "                self.ry_1_11(q_device, wires=11)\n",
    "                self.ry_1_12(q_device, wires=12)\n",
    "                self.ry_1_13(q_device, wires=13)\n",
    "                self.ry_1_14(q_device, wires=14)\n",
    "                self.ry_1_15(q_device, wires=15)\n",
    "            if self.n_wires >= 32:\n",
    "                self.ry_1_16(q_device, wires=16)\n",
    "                self.ry_1_17(q_device, wires=17)\n",
    "                self.ry_1_18(q_device, wires=18)\n",
    "                self.ry_1_19(q_device, wires=19)\n",
    "                self.ry_1_20(q_device, wires=20)\n",
    "                self.ry_1_21(q_device, wires=21)\n",
    "                self.ry_1_22(q_device, wires=22)\n",
    "                self.ry_1_23(q_device, wires=23)\n",
    "                self.ry_1_24(q_device, wires=24)\n",
    "                self.ry_1_25(q_device, wires=25)\n",
    "                self.ry_1_26(q_device, wires=26)\n",
    "                self.ry_1_27(q_device, wires=27)\n",
    "                self.ry_1_28(q_device, wires=26)\n",
    "                self.ry_1_29(q_device, wires=29)\n",
    "                self.ry_1_30(q_device, wires=30)\n",
    "                self.ry_1_31(q_device, wires=31)\n",
    "\n",
    "        def ansatz_gate_forward_ry_2(self, q_device):\n",
    "            if self.n_wires >= 2:\n",
    "                self.ry_2_0(q_device, wires=0)\n",
    "                self.ry_2_1(q_device, wires=1)\n",
    "            if self.n_wires >= 4:\n",
    "                self.ry_2_2(q_device, wires=2)\n",
    "                self.ry_2_3(q_device, wires=3)\n",
    "            if self.n_wires >= 8:\n",
    "                self.ry_2_4(q_device, wires=4)\n",
    "                self.ry_2_5(q_device, wires=5)\n",
    "                self.ry_2_6(q_device, wires=6)\n",
    "                self.ry_2_7(q_device, wires=7)\n",
    "            if self.n_wires >= 16:\n",
    "                self.ry_2_8(q_device, wires=8)\n",
    "                self.ry_2_9(q_device, wires=9)\n",
    "                self.ry_2_10(q_device, wires=10)\n",
    "                self.ry_2_11(q_device, wires=11)\n",
    "                self.ry_2_12(q_device, wires=12)\n",
    "                self.ry_2_13(q_device, wires=13)\n",
    "                self.ry_2_14(q_device, wires=14)\n",
    "                self.ry_2_15(q_device, wires=15)\n",
    "            if self.n_wires >= 32:\n",
    "                self.ry_2_16(q_device, wires=16)\n",
    "                self.ry_2_17(q_device, wires=17)\n",
    "                self.ry_2_18(q_device, wires=18)\n",
    "                self.ry_2_19(q_device, wires=19)\n",
    "                self.ry_2_20(q_device, wires=20)\n",
    "                self.ry_2_21(q_device, wires=21)\n",
    "                self.ry_2_22(q_device, wires=22)\n",
    "                self.ry_2_23(q_device, wires=23)\n",
    "                self.ry_2_24(q_device, wires=24)\n",
    "                self.ry_2_25(q_device, wires=25)\n",
    "                self.ry_2_26(q_device, wires=26)\n",
    "                self.ry_2_27(q_device, wires=27)\n",
    "                self.ry_2_28(q_device, wires=26)\n",
    "                self.ry_2_29(q_device, wires=29)\n",
    "                self.ry_2_30(q_device, wires=30)\n",
    "                self.ry_2_31(q_device, wires=31)\n",
    "        \n",
    "        @tq.static_support\n",
    "        def forward(self, q_device, x, return_q_device=False):\n",
    "            self.encoder(q_device, x)\n",
    "\n",
    "            self.ansatz_gate_forward_rx(q_device)\n",
    "            self.ansatz_gate_forward_ry_1(q_device)\n",
    "\n",
    "            for k in range(self.n_wires):\n",
    "                if k==self.n_wires-1:\n",
    "                    tqf.cnot(q_device, wires=[k, 0], static=self.static_mode, parent_graph=self.graph) \n",
    "                else:\n",
    "                    tqf.cnot(q_device, wires=[k, k+1], static=self.static_mode, parent_graph=self.graph)\n",
    "\n",
    "            self.ansatz_gate_forward_ry_2(q_device)\n",
    "\n",
    "            q_device = q_device.bfloat16()\n",
    "            \n",
    "            if return_q_device:\n",
    "                return q_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionQuantum(MultiHeadAttentionBase):\n",
    "    \n",
    "            \n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 dropout=0.1,\n",
    "                 mask=None,\n",
    "                 use_bias=False,\n",
    "                 n_qubits: int = 4,\n",
    "                 n_qlayers: int = 1,\n",
    "                 nb_shots=1024,\n",
    "                 q_device=\"default.qubit\"):\n",
    "        super(MultiHeadAttentionQuantum, self).__init__(embed_dim, num_heads, dropout=dropout, mask=mask, use_bias=use_bias)\n",
    "        \n",
    "        # todo: add intermediate layer to \"dress\" quantum circuit\n",
    "        assert n_qubits == embed_dim, \"Number of qubits ({n_qubits}) does not match embedding dim ({embed_dim})\"\n",
    "        self.n_qubits = n_qubits\n",
    "        #self.n_qlayers = n_qlayers\n",
    "\n",
    "        self.k_observables = [\"ZX\" *(n_qubits//2)]\n",
    "        self.q_observables = [\"ZX\" *(n_qubits//2)]\n",
    "        self.v_observables = [\"I\"*i + \"ZX\"+ \"I\"*(n_qubits - 2 - i) for i in range(n_qubits - 1)] + [\"XX\" + \"I\"*(n_qubits - 2)]\n",
    "\n",
    "        #self.k_observables_reversed = [\"\".join(reversed(obs)) for obs in self.k_observables]\n",
    "        #self.q_observables_reversed = [\"\".join(reversed(obs)) for obs in self.q_observables]\n",
    "        #self.v_observables_reversed = [\"\".join(reversed(obs)) for obs in self.v_observables]\n",
    "\n",
    "        self.k_layer = QLayer(n_qubits)\n",
    "        self.q_layer = QLayer(n_qubits)\n",
    "        self.v_layer = QLayer(n_qubits)\n",
    "        #self.measure = tq.MeasureAll(tq.PauliZ)\n",
    "        self.q_device = q_device\n",
    "        self.nb_shots = nb_shots\n",
    "\n",
    "    def get_exp_from_observables(self, x, quantum_layer, observables, session=None, return_quant_exec_time = False):\n",
    "        \n",
    "        q_dev = tq.QuantumDevice(n_wires=self.n_qubits, device=self.q_device, bsz=x.shape[0])\n",
    "\n",
    "        if session is not None:\n",
    "            options = Options(optimization_level=1, execution={\"shots\":self.nb_shots})\n",
    "            estimator = Estimator(session=session, options=options)\n",
    "\n",
    "            all_batch = []\n",
    "            all_time = []\n",
    "            for i in range(x.shape[0]):\n",
    "                job = estimator.run(circuits=[tq2qiskit(q_device=q_dev, m=quantum_layer, x=torch.unsqueeze(x[i], dim=0)) for o in range(len(observables))],\n",
    "                                     observables=observables)\n",
    "                all_batch.append(job.result().values)\n",
    "                all_time.append(job.usage_estimation)\n",
    "            \n",
    "            all_batch = torch.tensor(all_batch).float()\n",
    "            \n",
    "            if return_quant_exec_time:\n",
    "                return all_batch, all_time\n",
    "            \n",
    "            else:\n",
    "                return all_batch\n",
    "            \n",
    "        else:\n",
    "\n",
    "            if len(observables) > 1:\n",
    "\n",
    "                observables_reversed = [\"\".join(reversed(obs)) for obs in observables]\n",
    "                re_order_dict = {}\n",
    "                expval = expval_joint_sampling_grouping(qdev=quantum_layer(q_dev, x, return_q_device=True), observables=observables_reversed, n_shots_per_group=self.nb_shots)\n",
    "                \n",
    "                for obs, value in expval.items():\n",
    "                    re_order_dict[\"\".join(reversed(obs))] = value\n",
    "\n",
    "                print([re_order_dict[obs] for obs in observables])\n",
    "                return torch.stack([re_order_dict[obs] for obs in observables], dim=-1).float() # dim : [bsz, embed_dim]\n",
    "            else:\n",
    "\n",
    "                observable_reversed = \"\".join(reversed(observables[0]))\n",
    "                return expval_joint_sampling(qdev=quantum_layer(q_dev, x, return_q_device=True), observable=observable_reversed, n_shots=self.nb_shots).reshape([x.shape[0],-1]).float() # dim : [bsz, embed_dim]\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None, session=None):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        assert embed_dim == self.embed_dim, f\"Input embedding ({embed_dim}) does not match layer embedding size ({self.embed_dim})\"\n",
    "\n",
    "        v_exp_val = []\n",
    "        k_exp_val = []\n",
    "        q_exp_val = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "\n",
    "            v_exp_val.append(self.get_exp_from_observables(x=x[:, t, :].clone(), quantum_layer=self.v_layer, observables=self.v_observables, session=session))\n",
    "            k_exp_val.append(self.get_exp_from_observables(x=x[:, t, :].clone(), quantum_layer=self.k_layer, observables=self.k_observables, session=session))\n",
    "            q_exp_val.append(self.get_exp_from_observables(x=x[:, t, :].clone(), quantum_layer=self.q_layer, observables=self.q_observables, session=session))\n",
    "        \n",
    "        print(v_exp_val)\n",
    "        print(k_exp_val)\n",
    "        print(q_exp_val)\n",
    "\n",
    "        V = torch.transpose(torch.stack(v_exp_val), 0, 1)\n",
    "\n",
    "        K = torch.squeeze(torch.transpose(torch.stack(k_exp_val), 0, 1), dim= -1)\n",
    "        K = torch.exp((torch.pi / 2) * torch.complex(torch.zeros([batch_size, seq_len], dtype=torch.float32), K))\n",
    "\n",
    "        Q = torch.squeeze(torch.transpose(torch.stack(q_exp_val), 0, 1), dim= -1)\n",
    "        Q = torch.exp( - (torch.pi / 2) * torch.complex(torch.zeros([batch_size, seq_len], dtype=torch.float32), Q))\n",
    "         \n",
    "        print(V)\n",
    "        print(K)\n",
    "        print(Q)\n",
    "        #print(torch.unsqueeze(Q, dim=0) * torch.unsqueeze(K, dim=0).T)\n",
    "        #print((torch.unsqueeze(Q, dim=0) * torch.unsqueeze(K, dim=0).T).real @ V)\n",
    "\n",
    "        raise RuntimeError(\"STOP\")\n",
    "\n",
    "            \n",
    "            #print(torch.stack([torch.stack(list(expval_joint_sampling_grouping(qdev=self.v_layer(q_dev, x[:, t, :].clone(), return_q_device=True), observables=self.v_observables, n_shots_per_group=1024).values())) for t in range(seq_len)]))\n",
    "            #V = [self.v_layer(q_dev, x[:, t, :].clone()).values for t in range(seq_len)]\n",
    "        \n",
    "        #print(V)\n",
    "        \n",
    "        #K = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\n",
    "        #Q = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\n",
    "        #V = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\n",
    "\n",
    "        K = torch.Tensor(pad_sequence(K))\n",
    "        Q = torch.Tensor(pad_sequence(Q))\n",
    "        V = torch.Tensor(pad_sequence(V))\n",
    "        x = self.downstream(Q, K, V, batch_size, mask)\n",
    "        #output = [self.q_layer(x[:, t, :],q_dev) for t in range(seq_len)]\n",
    "        #output = torch.Tensor(pad_sequence(output)).clone()\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 4\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "SEQ_LEN = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.tensor(np.random.rand(BATCH_SIZE, SEQ_LEN, EMBED_DIM), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "classical_module = MultiHeadAttentionClassical(embed_dim=EMBED_DIM, num_heads=1, dropout=0.0)\n",
    "#quantum_module = MultiHeadAttentionQuantum(embed_dim=EMBED_DIM, num_heads=4, dropout=0.0, n_qubits=EMBED_DIM, q_device=\"cuda\", session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = classical_module(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantum_module = MultiHeadAttentionQuantum(embed_dim=EMBED_DIM, num_heads=1, dropout=0.0, n_qubits=EMBED_DIM, q_device=\"cpu\", nb_shots=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0.0137, 0.0117]), tensor([-0.0879, -0.0879]), tensor([-0.8105, -0.8262]), tensor([-0.1270,  0.0625])]\n",
      "[tensor([-0.0430,  0.0273]), tensor([-0.0664, -0.0840]), tensor([-0.7324, -0.6875]), tensor([ 0.2734, -0.1016])]\n",
      "[tensor([-0.0469, -0.0195]), tensor([-0.1562, -0.0762]), tensor([-0.5684, -0.7148]), tensor([ 0.3984, -0.1504])]\n",
      "[tensor([[ 0.0137, -0.0879, -0.8105, -0.1270],\n",
      "        [ 0.0117, -0.0879, -0.8262,  0.0625]]), tensor([[-0.0430, -0.0664, -0.7324,  0.2734],\n",
      "        [ 0.0273, -0.0840, -0.6875, -0.1016]]), tensor([[-0.0469, -0.1562, -0.5684,  0.3984],\n",
      "        [-0.0195, -0.0762, -0.7148, -0.1504]])]\n",
      "[tensor([[-0.1094],\n",
      "        [-0.2637]]), tensor([[-0.2461],\n",
      "        [-0.1055]]), tensor([[-0.0879],\n",
      "        [-0.2637]])]\n",
      "[tensor([[-0.0449],\n",
      "        [-0.0020]]), tensor([[ 0.0039],\n",
      "        [-0.0195]]), tensor([[0.0078],\n",
      "        [0.0469]])]\n",
      "tensor([[[ 0.0137, -0.0879, -0.8105, -0.1270],\n",
      "         [-0.0430, -0.0664, -0.7324,  0.2734],\n",
      "         [-0.0469, -0.1562, -0.5684,  0.3984]],\n",
      "\n",
      "        [[ 0.0117, -0.0879, -0.8262,  0.0625],\n",
      "         [ 0.0273, -0.0840, -0.6875, -0.1016],\n",
      "         [-0.0195, -0.0762, -0.7148, -0.1504]]])\n",
      "tensor([[0.9853-0.1710j, 0.9262-0.3770j, 0.9905-0.1376j],\n",
      "        [0.9154-0.4024j, 0.9863-0.1649j, 0.9154-0.4024j]])\n",
      "tensor([[0.9975+0.0705j, 1.0000-0.0061j, 0.9999-0.0123j],\n",
      "        [1.0000+0.0031j, 0.9995+0.0307j, 0.9973-0.0736j]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "STOP",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m output_q \u001b[39m=\u001b[39m quantum_module(test_input)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_quantum/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_quantum/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X42sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39mprint\u001b[39m(Q)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X42sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m \u001b[39m#print(torch.unsqueeze(Q, dim=0) * torch.unsqueeze(K, dim=0).T)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X42sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m \u001b[39m#print((torch.unsqueeze(Q, dim=0) * torch.unsqueeze(K, dim=0).T).real @ V)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X42sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSTOP\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X42sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m     \u001b[39m#print(torch.stack([torch.stack(list(expval_joint_sampling_grouping(qdev=self.v_layer(q_dev, x[:, t, :].clone(), return_q_device=True), observables=self.v_observables, n_shots_per_group=1024).values())) for t in range(seq_len)]))\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X42sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m     \u001b[39m#V = [self.v_layer(q_dev, x[:, t, :].clone()).values for t in range(seq_len)]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X42sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X42sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m \u001b[39m#Q = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X42sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m \u001b[39m#V = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X42sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m K \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(pad_sequence(K))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: STOP"
     ]
    }
   ],
   "source": [
    "output_q = quantum_module(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6260/1027062926.py:47: DeprecationWarning: Circuits that do not match the target hardware definition will no longer be supported after March 1, 2024. See the transpilation documentation (https://docs.quantum.ibm.com/transpile) for instructions to transform circuits and the primitive examples (https://docs.quantum.ibm.com/run/primitives-examples) to see this coupled with operator transformations.\n",
      "  job = estimator.run(circuits=[tq2qiskit(q_device=q_dev, m=quantum_layer, x=torch.unsqueeze(x[i], dim=0)) for o in range(len(observables))],\n",
      "/home/jesshuan/miniconda3/envs/torch_quantum/lib/python3.12/site-packages/qiskit_ibm_runtime/qiskit_runtime_service.py:935: UserWarning: Cloud simulators have been deprecated and will be removed on 15 May 2024. Use the new local testing mode in qiskit-ibm-runtime version 0.22.0 or later to meet your debugging needs.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 0.0098, -0.0957, -0.8359, -0.1680],\n",
      "        [-0.0117, -0.0625, -0.8262,  0.0918]]), tensor([[-0.0312, -0.0566, -0.7070,  0.3027],\n",
      "        [ 0.0254, -0.0234, -0.6504, -0.0742]]), tensor([[-0.0449, -0.1055, -0.5586,  0.3535],\n",
      "        [ 0.0547, -0.0605, -0.7441, -0.1270]])]\n",
      "[tensor([[-0.0664],\n",
      "        [-0.2734]]), tensor([[-0.2441],\n",
      "        [-0.0391]]), tensor([[-0.0605],\n",
      "        [-0.2148]])]\n",
      "[tensor([[ 0.0410],\n",
      "        [-0.0293]]), tensor([[-0.0430],\n",
      "        [ 0.0293]]), tensor([[-0.0215],\n",
      "        [ 0.1094]])]\n",
      "tensor([[[ 0.0098, -0.0957, -0.8359, -0.1680],\n",
      "         [-0.0312, -0.0566, -0.7070,  0.3027],\n",
      "         [-0.0449, -0.1055, -0.5586,  0.3535]],\n",
      "\n",
      "        [[-0.0117, -0.0625, -0.8262,  0.0918],\n",
      "         [ 0.0254, -0.0234, -0.6504, -0.0742],\n",
      "         [ 0.0547, -0.0605, -0.7441, -0.1270]]])\n",
      "tensor([[0.9946-0.1041j, 0.9274-0.3742j, 0.9955-0.0950j],\n",
      "        [0.9092-0.4164j, 0.9981-0.0613j, 0.9436-0.3311j]])\n",
      "tensor([[0.9979-0.0644j, 0.9977+0.0674j, 0.9994+0.0337j],\n",
      "        [0.9989+0.0460j, 0.9989-0.0460j, 0.9853-0.1710j]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "STOP",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb Cell 19\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m Session(backend\u001b[39m=\u001b[39mbackend) \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m#quantum_module = MultiHeadAttentionQuantum(embed_dim=EMBED_DIM, num_heads=1, dropout=0.0, n_qubits=EMBED_DIM, q_device=\"cpu\", session=session)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     output_q \u001b[39m=\u001b[39m quantum_module(test_input, session\u001b[39m=\u001b[39;49msession)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_quantum/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_quantum/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X43sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39mprint\u001b[39m(Q)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X43sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m \u001b[39m#print(torch.unsqueeze(Q, dim=0) * torch.unsqueeze(K, dim=0).T)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X43sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m \u001b[39m#print((torch.unsqueeze(Q, dim=0) * torch.unsqueeze(K, dim=0).T).real @ V)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X43sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSTOP\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X43sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m     \u001b[39m#print(torch.stack([torch.stack(list(expval_joint_sampling_grouping(qdev=self.v_layer(q_dev, x[:, t, :].clone(), return_q_device=True), observables=self.v_observables, n_shots_per_group=1024).values())) for t in range(seq_len)]))\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X43sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m     \u001b[39m#V = [self.v_layer(q_dev, x[:, t, :].clone()).values for t in range(seq_len)]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X43sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X43sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m \u001b[39m#Q = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X43sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m \u001b[39m#V = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/quantum_retnet_main_layer_poc.ipynb#X43sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m K \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(pad_sequence(K))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: STOP"
     ]
    }
   ],
   "source": [
    "with Session(backend=backend) as session:\n",
    "    #quantum_module = MultiHeadAttentionQuantum(embed_dim=EMBED_DIM, num_heads=1, dropout=0.0, n_qubits=EMBED_DIM, q_device=\"cpu\", session=session)\n",
    "    output_q = quantum_module(test_input, session=session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retnet_experiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
