{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qiskit                        0.46.1\n",
      "qiskit-aer                    0.13.3\n",
      "qiskit-ibm-runtime            0.20.0\n",
      "qiskit-terra                  0.46.1\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep qiskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_ibm_runtime import QiskitRuntimeService, Options, Sampler, Session, Estimator\n",
    "from qiskit import QuantumCircuit\n",
    "\n",
    "from qiskit_aer import AerSimulator\n",
    "\n",
    "\n",
    "from qiskit import IBMQ, Aer\n",
    "\n",
    "from qiskit_ibm_runtime.fake_provider import FakeManilaV2\n",
    "\n",
    "from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager\n",
    "\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "\n",
    "from torchquantum.measurement import expval_joint_analytical\n",
    "\n",
    "from torchquantum.measurement import expval_joint_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = QiskitRuntimeService(channel=\"ibm_quantum\", token=config[\"IBM_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<IBMBackend('simulator_extended_stabilizer')>,\n",
       " <IBMBackend('simulator_mps')>,\n",
       " <IBMBackend('simulator_statevector')>,\n",
       " <IBMBackend('simulator_stabilizer')>,\n",
       " <IBMBackend('ibm_brisbane')>,\n",
       " <IBMBackend('ibm_kyoto')>,\n",
       " <IBMBackend('ibm_osaka')>,\n",
       " <IBMBackend('ibmq_qasm_simulator')>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service.backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#service = QiskitRuntimeService()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = service.backend(\"ibmq_qasm_simulator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pdb\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchquantum as tq\n",
    "import qiskit_aer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchquantum.plugin.qiskit import tq2qiskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchquantum.functional as tqf\n",
    "import argparse\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "#import torchtext.legacy\n",
    "#from torchtext.legacy import data, datasets, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBase(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 mask=None,\n",
    "                 use_bias=False):\n",
    "        super(MultiHeadAttentionBase, self).__init__()\n",
    "\n",
    "        assert embed_dim % num_heads == 0, f\"Embedding dimension ({embed_dim}) should be divisible by number of heads ({num_heads})\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embed_dim // num_heads  # projection dimensions\n",
    "        self.k_linear = None\n",
    "        self.q_linear = None\n",
    "        self.v_linear = None\n",
    "        self.combine_heads = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn_weights = None\n",
    "    \n",
    "    def separate_heads(self, x):\n",
    "        '''\n",
    "        split into N heads\n",
    "        from (batch_size, seq_len, embed_dim)\n",
    "        to   (batch_size, seq_len, num_heads, embed_dim)\n",
    "        then transpose (1,2) to (batch_size, num_heads, seq_len, embed_dim)\n",
    "        to make mat mult straightforward for each head\n",
    "        '''\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    def attention(self, query, key, value, mask=None, dropout=None):\n",
    "        '''\n",
    "        Attention(Q, K, V) = softmax(Q K^T / sqrt(d_k))V\n",
    "        '''\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        # see also: https://tensorchiefs.github.io/dlday2018/tutorial/einsum.html\n",
    "        #scores = torch.einsum('bijh, bkjh -> bikh', query, key) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "        attn = torch.matmul(scores, value)\n",
    "        return attn, scores\n",
    "    \n",
    "    def downstream(self, query, key, value, batch_size, mask=None):\n",
    "        Q = self.separate_heads(query)\n",
    "        K = self.separate_heads(key)\n",
    "        V = self.separate_heads(value)\n",
    "\n",
    "        x, self.attn_weights = self.attention(Q, K, V, mask, dropout=self.dropout)\n",
    "\n",
    "        concat = x.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
    "\n",
    "        return concat\n",
    "        # output = self.combine_heads(concat)\n",
    "        # return output\n",
    "\n",
    "   # def forward(self, x, mask=None):\n",
    "    #    raise NotImplementedError(\"Base class does not execute forward function.\")\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionClassical(MultiHeadAttentionBase):\n",
    "    \n",
    "    def __init__(self, embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 dropout=0.1,\n",
    "                 mask=None,\n",
    "                 use_bias=False):\n",
    "        super(MultiHeadAttentionClassical, self).__init__(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, mask=mask, use_bias=use_bias)\n",
    "\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
    "        self.combine_heads = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        assert embed_dim == self.embed_dim, f\"Input embedding ({embed_dim}) does not match layer embedding size ({self.embed_dim})\"\n",
    "\n",
    "        K = self.k_linear(x)\n",
    "        Q = self.q_linear(x)\n",
    "        V = self.v_linear(x)\n",
    "\n",
    "        x = self.downstream(Q, K, V, batch_size, mask)\n",
    "        output = self.combine_heads(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLayer(tq.QuantumModule):\n",
    "        def __init__(self, n_qbits, *args, **kwargs):\n",
    "            super().__init__()    \n",
    "            self.n_wires = n_qbits\n",
    "            self.encoder = tq.GeneralEncoder(\n",
    "                    [{'input_idx': [i], 'func': 'rx', 'wires': [i]} for i in range(self.n_wires)])\n",
    "            #self.rx_list = [tq.RX(has_params=True, trainable=True) for _ in range(self.n_wires)]\n",
    "            #self.ry_test = tq.RY(has_params=True, trainable=True)\n",
    "            #self.measure = tq.MeasureAll(tq.PauliZ)\n",
    "            if n_qbits >= 2:\n",
    "                self.rx_0 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_1 = tq.RX(has_params=True, trainable=True)\n",
    "            if n_qbits >= 4:\n",
    "                self.rx_2 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_3 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "        def ansatz_gate_forward(self, q_device):\n",
    "            if self.n_wires >= 2:\n",
    "                self.rx_0(q_device, wires=0)\n",
    "                self.rx_1(q_device, wires=1)\n",
    "            if self.n_wires >= 4:\n",
    "                self.rx_2(q_device, wires=2)\n",
    "                self.rx_3(q_device, wires=3)\n",
    "\n",
    "        @tq.static_support\n",
    "        def forward(self, q_device, x):\n",
    "            self.encoder(q_device, x)\n",
    "            #for k in range(self.n_wires):\n",
    "                 #self.rx_list[k](q_device, wires=k)\n",
    "            #self.ry_test(q_device, wires=0)\n",
    "            self.ansatz_gate_forward(q_device)\n",
    "\n",
    "            for k in range(self.n_wires):\n",
    "                if k==self.n_wires-1:\n",
    "                    tqf.cnot(q_device, wires=[k, 0], static=self.static_mode, parent_graph=self.graph) \n",
    "                else:\n",
    "                    tqf.cnot(q_device, wires=[k, k+1], static=self.static_mode, parent_graph=self.graph)\n",
    "            q_device = q_device.bfloat16()\n",
    "            #output = tq.MeasureAll(tqf.PauliZ)(q_device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_layer = QLayer(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = torch.tensor(np.random.rand(1, 4), dtype=torch.float32)\n",
    "x_2 = torch.tensor(np.random.rand(1, 4), dtype=torch.float32)\n",
    "x_3 = torch.tensor(np.random.rand(1, 4), dtype=torch.float32)\n",
    "q_dev = tq.QuantumDevice(n_wires=4, device=\"cpu\", bsz=x_1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit_1 = tq2qiskit(q_device=q_dev, m=q_layer, x=x_1)\n",
    "circuit_2 = tq2qiskit(q_device=q_dev, m=q_layer, x=x_2)\n",
    "circuit_3 = tq2qiskit(q_device=q_dev, m=q_layer, x=x_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">     ┌─────────────┐  ┌────────────┐               ┌───┐\n",
       "q_0: ┤ Rx(0.48942) ├──┤ Rx(1.9407) ├──■────────────┤ X ├\n",
       "     ├─────────────┤ ┌┴────────────┤┌─┴─┐          └─┬─┘\n",
       "q_1: ┤ Rx(0.70358) ├─┤ Rx(0.98477) ├┤ X ├──■─────────┼──\n",
       "     ├─────────────┤ └┬────────────┤└───┘┌─┴─┐       │  \n",
       "q_2: ┤ Rx(0.90515) ├──┤ Rx(2.5582) ├─────┤ X ├──■────┼──\n",
       "     ├─────────────┴┐┌┴────────────┤     └───┘┌─┴─┐  │  \n",
       "q_3: ┤ Rx(0.023541) ├┤ Rx(-2.6929) ├──────────┤ X ├──■──\n",
       "     └──────────────┘└─────────────┘          └───┘     </pre>"
      ],
      "text/plain": [
       "     ┌─────────────┐  ┌────────────┐               ┌───┐\n",
       "q_0: ┤ Rx(0.48942) ├──┤ Rx(1.9407) ├──■────────────┤ X ├\n",
       "     ├─────────────┤ ┌┴────────────┤┌─┴─┐          └─┬─┘\n",
       "q_1: ┤ Rx(0.70358) ├─┤ Rx(0.98477) ├┤ X ├──■─────────┼──\n",
       "     ├─────────────┤ └┬────────────┤└───┘┌─┴─┐       │  \n",
       "q_2: ┤ Rx(0.90515) ├──┤ Rx(2.5582) ├─────┤ X ├──■────┼──\n",
       "     ├─────────────┴┐┌┴────────────┤     └───┘┌─┴─┐  │  \n",
       "q_3: ┤ Rx(0.023541) ├┤ Rx(-2.6929) ├──────────┤ X ├──■──\n",
       "     └──────────────┘└─────────────┘          └───┘     "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit_3.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"real_hardware = service.backend(\"ibm_brisbane\")\n",
    "aer = AerSimulator.from_backend(real_hardware)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options(optimization_level=1, execution={\"shots\":4000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">     ┌───┐\n",
       "q_0: ┤ H ├\n",
       "     └───┘\n",
       "q_1: ─────\n",
       "          \n",
       "c: 2/═════\n",
       "          </pre>"
      ],
      "text/plain": [
       "     ┌───┐\n",
       "q_0: ┤ H ├\n",
       "     └───┘\n",
       "q_1: ─────\n",
       "          \n",
       "c: 2/═════\n",
       "          "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qiskit import QuantumCircuit\n",
    " \n",
    "qc = QuantumCircuit(2, 2)\n",
    "qc.h(0)\n",
    "#qc.h(1)\n",
    "#qc.h(0)\n",
    "#qc.h(1)\n",
    "#qc.x(0)\n",
    "#qc.x(1)\n",
    "#qc.cnot(0,1)\n",
    "qc.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = generate_preset_pass_manager(backend=backend, optimization_level=1)\n",
    "circ_test = pm.run(qc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7862/457931810.py:7: DeprecationWarning: Circuits that do not match the target hardware definition will no longer be supported after March 1, 2024. See the transpilation documentation (https://docs.quantum.ibm.com/transpile) for instructions to transform circuits and the primitive examples (https://docs.quantum.ibm.com/run/primitives-examples) to see this coupled with operator transformations.\n",
      "  job = estimator.run(circuits=[circ_test, circ_test], observables=observables)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator results: EstimatorResult(values=array([ 1.  , -0.02]), metadata=[{'variance': 0.0, 'shots': 4000}, {'variance': 0.9996, 'shots': 4000}])\n"
     ]
    }
   ],
   "source": [
    "observables = [\"ZI\", \"IZ\"]\n",
    "\n",
    "\n",
    "with Session(service=service, backend=backend) as session:\n",
    "\n",
    "    estimator = Estimator(session=session, options=options)\n",
    "    job = estimator.run(circuits=[circ_test, circ_test], observables=observables)\n",
    "    print(f\"Estimator results: {job.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = generate_preset_pass_manager(backend=backend, optimization_level=1)\n",
    "circ_pc_1 = pm.run(circuit_1)\n",
    "circ_pc_2 = pm.run(circuit_2)\n",
    "circ_pc_3 = pm.run(circuit_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7694/679633775.py:13: DeprecationWarning: Circuits that do not match the target hardware definition will no longer be supported after March 1, 2024. See the transpilation documentation (https://docs.quantum.ibm.com/transpile) for instructions to transform circuits and the primitive examples (https://docs.quantum.ibm.com/run/primitives-examples) to see this coupled with operator transformations.\n",
      "  job = estimator.run(circuits=[circ_pc_1, circ_pc_1, circ_pc_1], observables=observables)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator results: EstimatorResult(values=array([-0.22  , -0.239 , -0.2145]), metadata=[{'variance': 0.9516, 'shots': 4000}, {'variance': 0.942879, 'shots': 4000}, {'variance': 0.95398975, 'shots': 4000}])\n"
     ]
    }
   ],
   "source": [
    "#observables = [\"ZZZZ\", \"ZZZZ\", \"ZZZZ\"]\n",
    "\n",
    "#H1 = SparsePauliOp.from_list([(\"ZIII\", 1), (\"IZII\", 2), (\"XIII\", 3)])\n",
    "\n",
    "#observables = [H1, H1, H1]\n",
    "\n",
    "observables = [\"ZIII\", \"ZIII\", \"ZIII\"]\n",
    "\n",
    "\n",
    "with Session(service=service, backend=backend) as session:\n",
    "\n",
    "    estimator = Estimator(session=session, options=options)\n",
    "    job = estimator.run(circuits=[circ_pc_1, circ_pc_1, circ_pc_1], observables=observables)\n",
    "    print(f\"Estimator results: {job.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.22  , -0.239 , -0.2145],\n",
       "       [-0.22  , -0.239 , -0.2145],\n",
       "       [-0.22  , -0.239 , -0.2145]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze([job.result().values for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.8046875 ,  0.        , -0.06640625],\n",
       "       [ 0.8046875 ,  0.        , -0.06640625],\n",
       "       [ 0.8046875 ,  0.        , -0.06640625]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze([job.result().values for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Session(service=service, backend=backend) as session:\n",
    "       # Submit a request to the Estimator primitive within the session.\n",
    "    estimator = Estimator(session=session, options=options)\n",
    "    job = estimator.run(circuits=[circ_pc], observables=[\"ZZZZ\"])\n",
    "    print(f\"Estimator results: {job.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionQuantum(MultiHeadAttentionBase):\n",
    "    \n",
    "            \n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 dropout=0.1,\n",
    "                 mask=None,\n",
    "                 use_bias=False,\n",
    "                 n_qubits: int = 4,\n",
    "                 n_qlayers: int = 1,\n",
    "                 q_device=\"default.qubit\",\n",
    "                 session: Session = None):\n",
    "        super(MultiHeadAttentionQuantum, self).__init__(embed_dim, num_heads, dropout=dropout, mask=mask, use_bias=use_bias)\n",
    "        \n",
    "        # todo: add intermediate layer to \"dress\" quantum circuit\n",
    "        assert n_qubits == embed_dim, \"Number of qubits ({n_qubits}) does not match embedding dim ({embed_dim})\"\n",
    "        self.n_qubits = n_qubits\n",
    "        #self.n_qlayers = n_qlayers\n",
    "        self.k_layer = QLayer(n_qubits)\n",
    "        self.q_layer = QLayer(n_qubits)\n",
    "        self.v_layer = QLayer(n_qubits)\n",
    "        #self.measure = tq.MeasureAll(tq.PauliZ)\n",
    "        self.q_device = q_device\n",
    "        self.session = session\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        assert embed_dim == self.embed_dim, f\"Input embedding ({embed_dim}) does not match layer embedding size ({self.embed_dim})\"\n",
    "\n",
    "        q_dev = tq.QuantumDevice(n_wires=self.n_qubits, device=self.q_device, bsz=x.shape[0])\n",
    "\n",
    "        options = Options(optimization_level=1, execution={})\n",
    "\n",
    "        observable = SparsePauliOp(\"Z\" * self.n_qubits)\n",
    "        estimator = Estimator(session=self.session, options=options)\n",
    "        \n",
    "        K = [estimator.run(circuits=[tq2qiskit(q_device=q_dev, m=self.k_layer, x=x[:, t, :].clone())],\n",
    "                                     observables=[observable]).result().values for t in range(seq_len)]\n",
    "        \n",
    "        Q = [estimator.run(circuits=[tq2qiskit(q_device=q_dev, m=self.q_layer, x=x[:, t, :].clone())],\n",
    "                                     observables=[observable]).result().values for t in range(seq_len)]\n",
    "        \n",
    "        V = [estimator.run(circuits=[tq2qiskit(q_device=q_dev, m=self.v_layer, x=x[:, t, :].clone())],\n",
    "                                     observables=[observable]).result().values for t in range(seq_len)]\n",
    "        \n",
    "        print(K)\n",
    "        \n",
    "        #K = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\n",
    "        #Q = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\n",
    "        #V = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\n",
    "\n",
    "        K = torch.Tensor(pad_sequence(K))\n",
    "        Q = torch.Tensor(pad_sequence(Q))\n",
    "        V = torch.Tensor(pad_sequence(V))\n",
    "        x = self.downstream(Q, K, V, batch_size, mask)\n",
    "        #output = [self.q_layer(x[:, t, :],q_dev) for t in range(seq_len)]\n",
    "        #output = torch.Tensor(pad_sequence(output)).clone()\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 4\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "SEQ_LEN = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "classical_module = MultiHeadAttentionClassical(embed_dim=EMBED_DIM, num_heads=4, dropout=0.0)\n",
    "#quantum_module = MultiHeadAttentionQuantum(embed_dim=EMBED_DIM, num_heads=4, dropout=0.0, n_qubits=EMBED_DIM, q_device=\"cuda\", session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.tensor(np.random.rand(BATCH_SIZE, SEQ_LEN, EMBED_DIM), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = classical_module(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = service.backend(\"ibmq_qasm_simulator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59203/1094969608.py:38: DeprecationWarning: Circuits that do not match the target hardware definition will no longer be supported after March 1, 2024. See the transpilation documentation (https://docs.quantum.ibm.com/transpile) for instructions to transform circuits and the primitive examples (https://docs.quantum.ibm.com/run/primitives-examples) to see this coupled with operator transformations.\n",
      "  K = [estimator.run(circuits=[tq2qiskit(q_device=q_dev, m=self.k_layer, x=x[:, t, :].clone())],\n",
      "/home/jesshuan/miniconda3/envs/torch_quantum/lib/python3.12/site-packages/qiskit_ibm_runtime/qiskit_runtime_service.py:935: UserWarning: Cloud simulators will be deprecated on 15 May 2024. Use the new local testing mode in qiskit-ibm-runtime version 0.22.0 or later to meet your debugging needs.\n",
      "  warnings.warn(warning_message)\n",
      "/tmp/ipykernel_59203/1094969608.py:41: DeprecationWarning: Circuits that do not match the target hardware definition will no longer be supported after March 1, 2024. See the transpilation documentation (https://docs.quantum.ibm.com/transpile) for instructions to transform circuits and the primitive examples (https://docs.quantum.ibm.com/run/primitives-examples) to see this coupled with operator transformations.\n",
      "  Q = [estimator.run(circuits=[tq2qiskit(q_device=q_dev, m=self.q_layer, x=x[:, t, :].clone())],\n",
      "/tmp/ipykernel_59203/1094969608.py:44: DeprecationWarning: Circuits that do not match the target hardware definition will no longer be supported after March 1, 2024. See the transpilation documentation (https://docs.quantum.ibm.com/transpile) for instructions to transform circuits and the primitive examples (https://docs.quantum.ibm.com/run/primitives-examples) to see this coupled with operator transformations.\n",
      "  V = [estimator.run(circuits=[tq2qiskit(q_device=q_dev, m=self.v_layer, x=x[:, t, :].clone())],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.61328125]), array([-0.95117188]), array([-0.52539062])]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer copy.ipynb Cell 29\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer%20copy.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m Session(service\u001b[39m=\u001b[39mservice, backend\u001b[39m=\u001b[39mbackend) \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer%20copy.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     quantum_module \u001b[39m=\u001b[39m MultiHeadAttentionQuantum(embed_dim\u001b[39m=\u001b[39mEMBED_DIM, num_heads\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, dropout\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, n_qubits\u001b[39m=\u001b[39mEMBED_DIM, q_device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m, session\u001b[39m=\u001b[39msession)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer%20copy.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     output_q \u001b[39m=\u001b[39m quantum_module(test_input)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_quantum/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_quantum/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer copy.ipynb Cell 29\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer%20copy.ipynb#X20sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mprint\u001b[39m(K)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer%20copy.ipynb#X20sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m#K = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer%20copy.ipynb#X20sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m#Q = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer%20copy.ipynb#X20sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m#V = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer%20copy.ipynb#X20sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m K \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(pad_sequence(K))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer%20copy.ipynb#X20sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m Q \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(pad_sequence(Q))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer%20copy.ipynb#X20sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m V \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(pad_sequence(V))\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_quantum/lib/python3.12/site-packages/torch/nn/utils/rnn.py:399\u001b[0m, in \u001b[0;36mpad_sequence\u001b[0;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[1;32m    395\u001b[0m         sequences \u001b[39m=\u001b[39m sequences\u001b[39m.\u001b[39munbind(\u001b[39m0\u001b[39m)\n\u001b[1;32m    397\u001b[0m \u001b[39m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[39m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mpad_sequence(sequences, batch_first, padding_value)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "with Session(service=service, backend=backend) as session:\n",
    "    quantum_module = MultiHeadAttentionQuantum(embed_dim=EMBED_DIM, num_heads=4, dropout=0.0, n_qubits=EMBED_DIM, q_device=\"cuda\", session=session)\n",
    "    output_q = quantum_module(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20, 8])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "QLayer.forward() missing 1 required positional argument: 'q_device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tq2qiskit(q_device\u001b[39m=\u001b[39;49mtq\u001b[39m.\u001b[39;49mQuantumDevice(n_wires\u001b[39m=\u001b[39;49mEMBED_DIM, bsz\u001b[39m=\u001b[39;49mBATCH_SIZE),m\u001b[39m=\u001b[39;49mq_layer)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_quantum/lib/python3.12/site-packages/torchquantum/plugin/qiskit/qiskit_plugin.py:417\u001b[0m, in \u001b[0;36mtq2qiskit\u001b[0;34m(q_device, m, x, draw, remove_ops, remove_ops_thres)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39m# forward to register all modules and parameters\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 417\u001b[0m     m\u001b[39m.\u001b[39;49mforward(q_device)\n\u001b[1;32m    418\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    419\u001b[0m     m\u001b[39m.\u001b[39mforward(q_device, x)\n",
      "\u001b[0;31mTypeError\u001b[0m: QLayer.forward() missing 1 required positional argument: 'q_device'"
     ]
    }
   ],
   "source": [
    "tq2qiskit(q_device=tq.QuantumDevice(n_wires=EMBED_DIM, bsz=BATCH_SIZE),m=q_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retnet_experiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
