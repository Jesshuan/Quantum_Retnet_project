{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qiskit                        0.46.1\n",
      "qiskit-aer                    0.13.3\n",
      "qiskit-ibm-runtime            0.20.0\n",
      "qiskit-terra                  0.46.1\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep qiskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_ibm_runtime import QiskitRuntimeService, Options, Sampler, Session, Estimator\n",
    "from qiskit import QuantumCircuit\n",
    "\n",
    "from qiskit_aer import AerSimulator\n",
    "\n",
    "\n",
    "from qiskit import IBMQ, Aer\n",
    "\n",
    "from qiskit_ibm_runtime.fake_provider import FakeManilaV2\n",
    "\n",
    "from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager\n",
    "\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "\n",
    "from torchquantum.measurement import expval_joint_analytical\n",
    "\n",
    "from torchquantum.measurement import expval_joint_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = QiskitRuntimeService(channel=\"ibm_quantum\", token=config[\"IBM_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<IBMBackend('simulator_stabilizer')>,\n",
       " <IBMBackend('ibm_brisbane')>,\n",
       " <IBMBackend('ibm_kyoto')>,\n",
       " <IBMBackend('ibm_osaka')>,\n",
       " <IBMBackend('ibmq_qasm_simulator')>,\n",
       " <IBMBackend('simulator_extended_stabilizer')>,\n",
       " <IBMBackend('simulator_mps')>,\n",
       " <IBMBackend('simulator_statevector')>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service.backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#service = QiskitRuntimeService()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = service.backend(\"ibmq_qasm_simulator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pdb\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchquantum as tq\n",
    "import qiskit_aer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchquantum.plugin.qiskit import tq2qiskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchquantum.functional as tqf\n",
    "import argparse\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "#import torchtext.legacy\n",
    "#from torchtext.legacy import data, datasets, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBase(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 mask=None,\n",
    "                 use_bias=False):\n",
    "        super(MultiHeadAttentionBase, self).__init__()\n",
    "\n",
    "        assert embed_dim % num_heads == 0, f\"Embedding dimension ({embed_dim}) should be divisible by number of heads ({num_heads})\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embed_dim // num_heads  # projection dimensions\n",
    "        self.k_linear = None\n",
    "        self.q_linear = None\n",
    "        self.v_linear = None\n",
    "        self.combine_heads = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn_weights = None\n",
    "    \n",
    "    def separate_heads(self, x):\n",
    "        '''\n",
    "        split into N heads\n",
    "        from (batch_size, seq_len, embed_dim)\n",
    "        to   (batch_size, seq_len, num_heads, embed_dim)\n",
    "        then transpose (1,2) to (batch_size, num_heads, seq_len, embed_dim)\n",
    "        to make mat mult straightforward for each head\n",
    "        '''\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    def attention(self, query, key, value, mask=None, dropout=None):\n",
    "        '''\n",
    "        Attention(Q, K, V) = softmax(Q K^T / sqrt(d_k))V\n",
    "        '''\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        # see also: https://tensorchiefs.github.io/dlday2018/tutorial/einsum.html\n",
    "        #scores = torch.einsum('bijh, bkjh -> bikh', query, key) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "        attn = torch.matmul(scores, value)\n",
    "        return attn, scores\n",
    "    \n",
    "    def downstream(self, query, key, value, batch_size, mask=None):\n",
    "        Q = self.separate_heads(query)\n",
    "        K = self.separate_heads(key)\n",
    "        V = self.separate_heads(value)\n",
    "\n",
    "        x, self.attn_weights = self.attention(Q, K, V, mask, dropout=self.dropout)\n",
    "\n",
    "        concat = x.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
    "\n",
    "        return concat\n",
    "        # output = self.combine_heads(concat)\n",
    "        # return output\n",
    "\n",
    "   # def forward(self, x, mask=None):\n",
    "    #    raise NotImplementedError(\"Base class does not execute forward function.\")\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionClassical(MultiHeadAttentionBase):\n",
    "    \n",
    "    def __init__(self, embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 dropout=0.1,\n",
    "                 mask=None,\n",
    "                 use_bias=False):\n",
    "        super(MultiHeadAttentionClassical, self).__init__(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, mask=mask, use_bias=use_bias)\n",
    "\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
    "        self.combine_heads = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        assert embed_dim == self.embed_dim, f\"Input embedding ({embed_dim}) does not match layer embedding size ({self.embed_dim})\"\n",
    "\n",
    "        K = self.k_linear(x)\n",
    "        Q = self.q_linear(x)\n",
    "        V = self.v_linear(x)\n",
    "\n",
    "        x = self.downstream(Q, K, V, batch_size, mask)\n",
    "        output = self.combine_heads(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLayer(tq.QuantumModule):\n",
    "        def __init__(self, n_qbits, *args, **kwargs):\n",
    "            super().__init__()    \n",
    "            self.n_wires = n_qbits\n",
    "            self.encoder = tq.GeneralEncoder(\n",
    "                    [{'input_idx': [i], 'func': 'rx', 'wires': [i]} for i in range(self.n_wires)])\n",
    "            #self.rx_list = [tq.RX(has_params=True, trainable=True) for _ in range(self.n_wires)]\n",
    "            #self.ry_test = tq.RY(has_params=True, trainable=True)\n",
    "            #self.measure = tq.MeasureAll(tq.PauliZ)\n",
    "            if n_qbits >= 2:\n",
    "                self.rx_0 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_1 = tq.RX(has_params=True, trainable=True)\n",
    "            if n_qbits >= 4:\n",
    "                self.rx_2 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_3 = tq.RX(has_params=True, trainable=True)\n",
    "            if n_qbits >= 8:\n",
    "                self.rx_4 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_5 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_6 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_7 = tq.RX(has_params=True, trainable=True)\n",
    "            if n_qbits >= 16:\n",
    "                self.rx_8 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_9 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_10 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_11 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_12 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_13 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_14 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_15 = tq.RX(has_params=True, trainable=True)\n",
    "            if n_qbits >= 32:\n",
    "                self.rx_16 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_17 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_18 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_19 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_20 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_21 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_22 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_23 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_24 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_25 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_26 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_27 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_28 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_29 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_30 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_31 = tq.RX(has_params=True, trainable=True)\n",
    "            \n",
    "            self.measure = tq.MeasureAll(obs=[\"ZI\", \"IZ\"])\n",
    "\n",
    "        def ansatz_gate_forward(self, q_device):\n",
    "            if self.n_wires >= 2:\n",
    "                self.rx_0(q_device, wires=0)\n",
    "                self.rx_1(q_device, wires=1)\n",
    "            if self.n_wires >= 4:\n",
    "                self.rx_2(q_device, wires=2)\n",
    "                self.rx_3(q_device, wires=3)\n",
    "            if self.n_wires >= 8:\n",
    "                self.rx_4(q_device, wires=4)\n",
    "                self.rx_5(q_device, wires=5)\n",
    "                self.rx_6(q_device, wires=6)\n",
    "                self.rx_7(q_device, wires=7)\n",
    "            if self.n_wires >= 16:\n",
    "                self.rx_8(q_device, wires=8)\n",
    "                self.rx_9(q_device, wires=9)\n",
    "                self.rx_10(q_device, wires=10)\n",
    "                self.rx_11(q_device, wires=11)\n",
    "                self.rx_12(q_device, wires=12)\n",
    "                self.rx_13(q_device, wires=13)\n",
    "                self.rx_14(q_device, wires=14)\n",
    "                self.rx_15(q_device, wires=15)\n",
    "            if self.n_wires >= 32:\n",
    "                self.rx_16(q_device, wires=16)\n",
    "                self.rx_17(q_device, wires=17)\n",
    "                self.rx_18(q_device, wires=18)\n",
    "                self.rx_19(q_device, wires=19)\n",
    "                self.rx_20(q_device, wires=20)\n",
    "                self.rx_21(q_device, wires=21)\n",
    "                self.rx_22(q_device, wires=22)\n",
    "                self.rx_23(q_device, wires=23)\n",
    "                self.rx_24(q_device, wires=24)\n",
    "                self.rx_25(q_device, wires=25)\n",
    "                self.rx_26(q_device, wires=26)\n",
    "                self.rx_27(q_device, wires=27)\n",
    "                self.rx_28(q_device, wires=26)\n",
    "                self.rx_29(q_device, wires=29)\n",
    "                self.rx_30(q_device, wires=30)\n",
    "                self.rx_31(q_device, wires=31)\n",
    "        \n",
    "       \n",
    "\n",
    "        @tq.static_support\n",
    "        def forward(self, q_device, x):\n",
    "            self.encoder(q_device, x)\n",
    "            #for k in range(self.n_wires):\n",
    "                 #self.rx_list[k](q_device, wires=k)\n",
    "            #self.ry_test(q_device, wires=0)\n",
    "            self.ansatz_gate_forward(q_device)\n",
    "\n",
    "            for k in range(self.n_wires):\n",
    "                if k==self.n_wires-1:\n",
    "                    tqf.cnot(q_device, wires=[k, 0], static=self.static_mode, parent_graph=self.graph) \n",
    "                else:\n",
    "                    tqf.cnot(q_device, wires=[k, k+1], static=self.static_mode, parent_graph=self.graph)\n",
    "            q_device = q_device.bfloat16()\n",
    "            return(self.measure(q_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_layer = QLayer(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = torch.tensor(np.random.rand(1, 16), dtype=torch.float32)\n",
    "x_2 = torch.tensor(np.random.rand(1, 16), dtype=torch.float32)\n",
    "x_3 = torch.tensor(np.random.rand(1, 16), dtype=torch.float32)\n",
    "q_dev = tq.QuantumDevice(n_wires=16, device=\"cpu\", bsz=x_1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m circuit_1 \u001b[38;5;241m=\u001b[39m \u001b[43mtq2qiskit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq_dev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m circuit_2 \u001b[38;5;241m=\u001b[39m tq2qiskit(q_device\u001b[38;5;241m=\u001b[39mq_dev, m\u001b[38;5;241m=\u001b[39mq_layer, x\u001b[38;5;241m=\u001b[39mx_2)\n\u001b[1;32m      3\u001b[0m circuit_3 \u001b[38;5;241m=\u001b[39m tq2qiskit(q_device\u001b[38;5;241m=\u001b[39mq_dev, m\u001b[38;5;241m=\u001b[39mq_layer, x\u001b[38;5;241m=\u001b[39mx_3)\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_test/lib/python3.12/site-packages/torchquantum/plugin/qiskit/qiskit_plugin.py:419\u001b[0m, in \u001b[0;36mtq2qiskit\u001b[0;34m(q_device, m, x, draw, remove_ops, remove_ops_thres)\u001b[0m\n\u001b[1;32m    417\u001b[0m     m\u001b[38;5;241m.\u001b[39mforward(q_device)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m m\u001b[38;5;241m.\u001b[39mis_graph_top \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    422\u001b[0m m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mbuild_flat_module_list()\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_test/lib/python3.12/site-packages/torchquantum/graph/graphs.py:73\u001b[0m, in \u001b[0;36mstatic_support.<locals>.forward_register_graph\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstatic_mode \u001b[38;5;129;01mand\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mparent_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mparent_graph\u001b[38;5;241m.\u001b[39madd_op(args[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 73\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstatic_mode \u001b[38;5;129;01mand\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mis_graph_top:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# finish build graph, set flag\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_graph_build_finish()\n",
      "Cell \u001b[0;32mIn[85], line 105\u001b[0m, in \u001b[0;36mQLayer.forward\u001b[0;34m(self, q_device, x)\u001b[0m\n\u001b[1;32m    103\u001b[0m         tqf\u001b[38;5;241m.\u001b[39mcnot(q_device, wires\u001b[38;5;241m=\u001b[39m[k, k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], static\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_mode, parent_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph)\n\u001b[1;32m    104\u001b[0m q_device \u001b[38;5;241m=\u001b[39m q_device\u001b[38;5;241m.\u001b[39mbfloat16()\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_device\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_test/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_test/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_test/lib/python3.12/site-packages/torchquantum/measurement/measurements.py:324\u001b[0m, in \u001b[0;36mMeasureAll.forward\u001b[0;34m(self, qdev)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, qdev: tq\u001b[38;5;241m.\u001b[39mQuantumDevice):\n\u001b[0;32m--> 324\u001b[0m     x \u001b[38;5;241m=\u001b[39m expval(qdev, \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(qdev\u001b[38;5;241m.\u001b[39mn_wires)), [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m*\u001b[39m qdev\u001b[38;5;241m.\u001b[39mn_wires)\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_c_reg_mapping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m         c2v_mapping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_c_reg_mapping[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc2v\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "circuit_1 = tq2qiskit(q_device=q_dev, m=q_layer, x=x_1)\n",
    "circuit_2 = tq2qiskit(q_device=q_dev, m=q_layer, x=x_2)\n",
    "circuit_3 = tq2qiskit(q_device=q_dev, m=q_layer, x=x_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"real_hardware = service.backend(\"ibm_brisbane\")\n",
    "aer = AerSimulator.from_backend(real_hardware)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options(optimization_level=1, execution={\"shots\":1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">     ┌───┐\n",
       "q_0: ┤ H ├\n",
       "     └───┘\n",
       "q_1: ─────\n",
       "          \n",
       "c: 2/═════\n",
       "          </pre>"
      ],
      "text/plain": [
       "     ┌───┐\n",
       "q_0: ┤ H ├\n",
       "     └───┘\n",
       "q_1: ─────\n",
       "          \n",
       "c: 2/═════\n",
       "          "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qiskit import QuantumCircuit\n",
    " \n",
    "qc = QuantumCircuit(2, 2)\n",
    "qc.h(0)\n",
    "#qc.h(1)\n",
    "#qc.h(0)\n",
    "#qc.h(1)\n",
    "#qc.x(0)\n",
    "#qc.x(1)\n",
    "#qc.cnot(0,1)\n",
    "qc.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = generate_preset_pass_manager(backend=backend, optimization_level=1)\n",
    "circ_test = pm.run(qc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "meas = tq.MeasureAll(obs=[\"ZI\", \"IZ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ZI', 'IZ']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tq.MeasureMultiPauliSum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7862/457931810.py:7: DeprecationWarning: Circuits that do not match the target hardware definition will no longer be supported after March 1, 2024. See the transpilation documentation (https://docs.quantum.ibm.com/transpile) for instructions to transform circuits and the primitive examples (https://docs.quantum.ibm.com/run/primitives-examples) to see this coupled with operator transformations.\n",
      "  job = estimator.run(circuits=[circ_test, circ_test], observables=observables)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator results: EstimatorResult(values=array([ 1.  , -0.02]), metadata=[{'variance': 0.0, 'shots': 4000}, {'variance': 0.9996, 'shots': 4000}])\n"
     ]
    }
   ],
   "source": [
    "observables = [\"ZI\", \"IZ\"]\n",
    "\n",
    "\n",
    "with Session(service=service, backend=backend) as session:\n",
    "\n",
    "    estimator = Estimator(session=session, options=options)\n",
    "    job = estimator.run(circuits=[circ_test, circ_test], observables=observables)\n",
    "    print(f\"Estimator results: {job.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = generate_preset_pass_manager(backend=backend, optimization_level=1)\n",
    "circ_pc_1 = pm.run(circuit_1)\n",
    "circ_pc_2 = pm.run(circuit_2)\n",
    "circ_pc_3 = pm.run(circuit_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7694/679633775.py:13: DeprecationWarning: Circuits that do not match the target hardware definition will no longer be supported after March 1, 2024. See the transpilation documentation (https://docs.quantum.ibm.com/transpile) for instructions to transform circuits and the primitive examples (https://docs.quantum.ibm.com/run/primitives-examples) to see this coupled with operator transformations.\n",
      "  job = estimator.run(circuits=[circ_pc_1, circ_pc_1, circ_pc_1], observables=observables)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator results: EstimatorResult(values=array([-0.22  , -0.239 , -0.2145]), metadata=[{'variance': 0.9516, 'shots': 4000}, {'variance': 0.942879, 'shots': 4000}, {'variance': 0.95398975, 'shots': 4000}])\n"
     ]
    }
   ],
   "source": [
    "#observables = [\"ZZZZ\", \"ZZZZ\", \"ZZZZ\"]\n",
    "\n",
    "#H1 = SparsePauliOp.from_list([(\"ZIII\", 1), (\"IZII\", 2), (\"XIII\", 3)])\n",
    "\n",
    "#observables = [H1, H1, H1]\n",
    "\n",
    "observables = [\"ZIII\", \"ZIII\", \"ZIII\"]\n",
    "\n",
    "\n",
    "with Session(service=service, backend=backend) as session:\n",
    "\n",
    "    estimator = Estimator(session=session, options=options)\n",
    "    job = estimator.run(circuits=[circ_pc_1, circ_pc_1, circ_pc_1], observables=observables)\n",
    "    print(f\"Estimator results: {job.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "observables = SparsePauliOp.from_list([(\"ZX\" + \"I\"*(14),1), (\"XY\" + \"I\"*(14),1)] + [(\"I\"*i + \"Z\" + \"I\"*(15-i), 1) for i in range(16)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionQuantum(MultiHeadAttentionBase):\n",
    "    \n",
    "            \n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 dropout=0.1,\n",
    "                 mask=None,\n",
    "                 use_bias=False,\n",
    "                 n_qubits: int = 4,\n",
    "                 n_qlayers: int = 1,\n",
    "                 q_device=\"default.qubit\",\n",
    "                 session = None):\n",
    "        super(MultiHeadAttentionQuantum, self).__init__(embed_dim, num_heads, dropout=dropout, mask=mask, use_bias=use_bias)\n",
    "        \n",
    "        # todo: add intermediate layer to \"dress\" quantum circuit\n",
    "        assert n_qubits == embed_dim, \"Number of qubits ({n_qubits}) does not match embedding dim ({embed_dim})\"\n",
    "        self.n_qubits = n_qubits\n",
    "        #self.n_qlayers = n_qlayers\n",
    "        self.k_layer = QLayer(n_qubits)\n",
    "        self.q_layer = QLayer(n_qubits)\n",
    "        self.v_layer = QLayer(n_qubits)\n",
    "        #self.measure = tq.MeasureAll(tq.PauliZ)\n",
    "        self.q_device = q_device\n",
    "        self.session = session\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        assert embed_dim == self.embed_dim, f\"Input embedding ({embed_dim}) does not match layer embedding size ({self.embed_dim})\"\n",
    "\n",
    "        q_dev = tq.QuantumDevice(n_wires=self.n_qubits, device=self.q_device, bsz=x.shape[0])\n",
    "\n",
    "        options = Options(optimization_level=1, execution={})\n",
    "\n",
    "        observables = SparsePauliOp.from_list([(\"ZX\" + \"I\"*(14),1), (\"XY\" + \"I\"*(14),1)] + [(\"I\"*i + \"Z\" + \"I\"*(15-i), 1) for i in range(16)])\n",
    "\n",
    "        if self.session is not None:\n",
    "            estimator = Estimator(session=self.session, options=options)\n",
    "            results = [estimator.run(circuits=[tq2qiskit(q_device=q_dev, m=self.k_layer, x=x[:, t, :].clone())],\n",
    "                                     observables=[observables]).result().values for t in range(seq_len)]\n",
    "            \n",
    "        else:\n",
    "\n",
    "            results = [self.q_layer(q_dev, x[:, t, :].clone()) for t in range(seq_len)]\n",
    "        \n",
    "        print(results)\n",
    "        \n",
    "        #K = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\n",
    "        #Q = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\n",
    "        #V = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\n",
    "\n",
    "        K = torch.Tensor(pad_sequence(K))\n",
    "        Q = torch.Tensor(pad_sequence(Q))\n",
    "        V = torch.Tensor(pad_sequence(V))\n",
    "        x = self.downstream(Q, K, V, batch_size, mask)\n",
    "        #output = [self.q_layer(x[:, t, :],q_dev) for t in range(seq_len)]\n",
    "        #output = torch.Tensor(pad_sequence(output)).clone()\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 16\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "SEQ_LEN = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "classical_module = MultiHeadAttentionClassical(embed_dim=EMBED_DIM, num_heads=4, dropout=0.0)\n",
    "#quantum_module = MultiHeadAttentionQuantum(embed_dim=EMBED_DIM, num_heads=4, dropout=0.0, n_qubits=EMBED_DIM, q_device=\"cuda\", session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.tensor(np.random.rand(BATCH_SIZE, SEQ_LEN, EMBED_DIM), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = classical_module(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 16])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantum_module = MultiHeadAttentionQuantum(embed_dim=EMBED_DIM, num_heads=4, dropout=0.0, n_qubits=EMBED_DIM, q_device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantum_module.session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output_q \u001b[38;5;241m=\u001b[39m \u001b[43mquantum_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_test/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_test/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[89], line 44\u001b[0m, in \u001b[0;36mMultiHeadAttentionQuantum.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     39\u001b[0m     results \u001b[38;5;241m=\u001b[39m [estimator\u001b[38;5;241m.\u001b[39mrun(circuits\u001b[38;5;241m=\u001b[39m[tq2qiskit(q_device\u001b[38;5;241m=\u001b[39mq_dev, m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_layer, x\u001b[38;5;241m=\u001b[39mx[:, t, :]\u001b[38;5;241m.\u001b[39mclone())],\n\u001b[1;32m     40\u001b[0m                              observables\u001b[38;5;241m=\u001b[39m[observables])\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len)]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_dev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len)]\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#K = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#Q = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#V = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_test/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_test/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_test/lib/python3.12/site-packages/torchquantum/graph/graphs.py:73\u001b[0m, in \u001b[0;36mstatic_support.<locals>.forward_register_graph\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstatic_mode \u001b[38;5;129;01mand\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mparent_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mparent_graph\u001b[38;5;241m.\u001b[39madd_op(args[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 73\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstatic_mode \u001b[38;5;129;01mand\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mis_graph_top:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# finish build graph, set flag\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_graph_build_finish()\n",
      "Cell \u001b[0;32mIn[85], line 105\u001b[0m, in \u001b[0;36mQLayer.forward\u001b[0;34m(self, q_device, x)\u001b[0m\n\u001b[1;32m    103\u001b[0m         tqf\u001b[38;5;241m.\u001b[39mcnot(q_device, wires\u001b[38;5;241m=\u001b[39m[k, k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], static\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_mode, parent_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph)\n\u001b[1;32m    104\u001b[0m q_device \u001b[38;5;241m=\u001b[39m q_device\u001b[38;5;241m.\u001b[39mbfloat16()\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_device\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_test/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_test/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_test/lib/python3.12/site-packages/torchquantum/measurement/measurements.py:324\u001b[0m, in \u001b[0;36mMeasureAll.forward\u001b[0;34m(self, qdev)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, qdev: tq\u001b[38;5;241m.\u001b[39mQuantumDevice):\n\u001b[0;32m--> 324\u001b[0m     x \u001b[38;5;241m=\u001b[39m expval(qdev, \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(qdev\u001b[38;5;241m.\u001b[39mn_wires)), [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m*\u001b[39m qdev\u001b[38;5;241m.\u001b[39mn_wires)\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_c_reg_mapping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m         c2v_mapping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_c_reg_mapping[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc2v\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "output_q = quantum_module(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "QiskitBackendNotFoundError",
     "evalue": "'No backend matches the criteria.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mQiskitBackendNotFoundError\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m      2\u001b[0m     quantum_module \u001b[38;5;241m=\u001b[39m MultiHeadAttentionQuantum(embed_dim\u001b[38;5;241m=\u001b[39mEMBED_DIM, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, n_qubits\u001b[38;5;241m=\u001b[39mEMBED_DIM, q_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, session\u001b[38;5;241m=\u001b[39msession)\n\u001b[1;32m      3\u001b[0m     output_q \u001b[38;5;241m=\u001b[39m quantum_module(test_input)\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_test/lib/python3.12/site-packages/qiskit_ibm_runtime/session.py:132\u001b[0m, in \u001b[0;36mSession.__init__\u001b[0;34m(self, service, backend, max_time)\u001b[0m\n\u001b[1;32m    130\u001b[0m     backend \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     backend_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_service\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_instance \u001b[38;5;241m=\u001b[39m backend_obj\u001b[38;5;241m.\u001b[39m_instance\n\u001b[1;32m    134\u001b[0m     sim_backend \u001b[38;5;241m=\u001b[39m backend_obj\u001b[38;5;241m.\u001b[39mconfiguration()\u001b[38;5;241m.\u001b[39msimulator\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_test/lib/python3.12/site-packages/qiskit_ibm_runtime/qiskit_runtime_service.py:838\u001b[0m, in \u001b[0;36mQiskitRuntimeService.backend\u001b[0;34m(self, name, instance)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a single backend matching the specified filtering.\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \n\u001b[1;32m    824\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;124;03m    QiskitBackendNotFoundError: if no backend could be found.\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;66;03m# pylint: disable=arguments-differ, line-too-long\u001b[39;00m\n\u001b[0;32m--> 838\u001b[0m backends \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m backends:\n\u001b[1;32m    840\u001b[0m     cloud_msg_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/retnet_test/lib/python3.12/site-packages/qiskit_ibm_runtime/qiskit_runtime_service.py:598\u001b[0m, in \u001b[0;36mQiskitRuntimeService.backends\u001b[0;34m(self, name, min_num_qubits, instance, dynamic_circuits, filters, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name:\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backends:\n\u001b[0;32m--> 598\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m QiskitBackendNotFoundError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo backend matches the criteria.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backends[name] \u001b[38;5;129;01mor\u001b[39;00m instance_filter \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backends[name]\u001b[38;5;241m.\u001b[39m_instance:\n\u001b[1;32m    600\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_backend_config(name)\n",
      "\u001b[0;31mQiskitBackendNotFoundError\u001b[0m: 'No backend matches the criteria.'"
     ]
    }
   ],
   "source": [
    "with Session(backend=aer) as session:\n",
    "    quantum_module = MultiHeadAttentionQuantum(embed_dim=EMBED_DIM, num_heads=4, dropout=0.0, n_qubits=EMBED_DIM, q_device=\"cuda\", session=session)\n",
    "    output_q = quantum_module(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20, 8])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_q.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retnet_experiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
