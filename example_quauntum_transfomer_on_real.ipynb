{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qiskit                        0.46.1\n",
      "qiskit-aer                    0.13.3\n",
      "qiskit-ibm-runtime            0.20.0\n",
      "qiskit-terra                  0.46.1\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep qiskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_ibm_runtime import QiskitRuntimeService, Options, Sampler, Session, Estimator\n",
    "from qiskit import QuantumCircuit\n",
    "\n",
    "from qiskit_aer import AerSimulator\n",
    "\n",
    "\n",
    "from qiskit import IBMQ, Aer\n",
    "\n",
    "from qiskit_ibm_runtime.fake_provider import FakeManilaV2\n",
    "\n",
    "from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager\n",
    "\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "\n",
    "from torchquantum.measurement import expval_joint_analytical\n",
    "\n",
    "from torchquantum.measurement import expval_joint_sampling, expval_joint_sampling_grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pdb\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchquantum as tq\n",
    "import qiskit_aer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchquantum.functional as tqf\n",
    "import argparse\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "#import torchtext.legacy\n",
    "#from torchtext.legacy import data, datasets, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchquantum.plugin.qiskit import tq2qiskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = QiskitRuntimeService(channel=\"ibm_quantum\", token=config[\"IBM_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options(optimization_level=1, execution={\"shots\":32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<IBMBackend('ibm_brisbane')>,\n",
       " <IBMBackend('ibm_kyoto')>,\n",
       " <IBMBackend('ibm_sherbrooke')>,\n",
       " <IBMBackend('ibmq_qasm_simulator')>,\n",
       " <IBMBackend('simulator_extended_stabilizer')>,\n",
       " <IBMBackend('simulator_stabilizer')>,\n",
       " <IBMBackend('ibm_osaka')>,\n",
       " <IBMBackend('simulator_mps')>,\n",
       " <IBMBackend('simulator_statevector')>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service.backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = service.backend(\"ibmq_qasm_simulator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Little test backend service and Estimator :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">     ┌───┐\n",
       "q_0: ┤ H ├\n",
       "     ├───┤\n",
       "q_1: ┤ X ├\n",
       "     └───┘\n",
       "c: 2/═════\n",
       "          </pre>"
      ],
      "text/plain": [
       "     ┌───┐\n",
       "q_0: ┤ H ├\n",
       "     ├───┤\n",
       "q_1: ┤ X ├\n",
       "     └───┘\n",
       "c: 2/═════\n",
       "          "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qiskit import QuantumCircuit\n",
    " \n",
    "qc = QuantumCircuit(2, 2)\n",
    "qc.h(0)\n",
    "#qc.h(1)\n",
    "#qc.h(0)\n",
    "#qc.h(1)\n",
    "#qc.x(0)\n",
    "qc.x(1)\n",
    "#qc.cnot(0,1)\n",
    "qc.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdev = tq.QuantumDevice(n_wires=2, bsz=1, device=\"cpu\") # use device='cuda' for GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TQTest(tq.QuantumModule):\n",
    "        def __init__(self, n_qubits, *args, **kwargs):\n",
    "            super().__init__()\n",
    "            self.n_wires = n_qubits\n",
    "            #self.rx_0 = tq.RX(has_params=True, trainable=True)\n",
    "        @tq.static_support\n",
    "        def forward(self, q_device, return_q_dev=False):\n",
    "            tqf.h(q_device, wires=0, static=self.static_mode, parent_graph=self.graph) \n",
    "            tqf.x(q_device, wires=1, static=self.static_mode, parent_graph=self.graph)\n",
    "            #self.rx_0(q_device, wires=0)\n",
    "             \n",
    "            q_device = q_device.bfloat16()\n",
    "\n",
    "            if return_q_dev:\n",
    "                 return q_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqtest = TQTest(n_qubits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in tqtest.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ZI': tensor([0.]), 'IZ': tensor([-1.])}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observables = [\"ZI\", \"IZ\"]\n",
    "expval_joint_sampling_grouping(qdev=tqtest(qdev, return_q_dev=True), observables=observables, n_shots_per_group=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_2 = tq2qiskit(q_device=qdev, m=tqtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">     ┌───┐\n",
       "q_0: ┤ H ├\n",
       "     ├───┤\n",
       "q_1: ┤ X ├\n",
       "     └───┘\n",
       "c: 2/═════\n",
       "          </pre>"
      ],
      "text/plain": [
       "     ┌───┐\n",
       "q_0: ┤ H ├\n",
       "     ├───┤\n",
       "q_1: ┤ X ├\n",
       "     └───┘\n",
       "c: 2/═════\n",
       "          "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qc.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">     ┌───┐┌────────────┐\n",
       "q_0: ┤ H ├┤ Rx(1.4349) ├\n",
       "     ├───┤└────────────┘\n",
       "q_1: ┤ X ├──────────────\n",
       "     └───┘              </pre>"
      ],
      "text/plain": [
       "     ┌───┐┌────────────┐\n",
       "q_0: ┤ H ├┤ Rx(1.4349) ├\n",
       "     ├───┤└────────────┘\n",
       "q_1: ┤ X ├──────────────\n",
       "     └───┘              "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qc_2.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observables = [\"ZI\", \"IZ\"]\n",
    "\n",
    "\n",
    "with Session(service=service, backend=backend) as session:\n",
    "\n",
    "    estimator = Estimator(session=session, options=options)\n",
    "    job = estimator.run(circuits=[qc, qc], observables=observables)\n",
    "    print(f\"Estimator results: {job.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = generate_preset_pass_manager(backend=backend, optimization_level=1)\n",
    "circ_test = pm.run(qc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">global phase: π/4\n",
       "     ┌─────────┐┌────┐┌─────────┐\n",
       "q_0: ┤ Rz(π/2) ├┤ √X ├┤ Rz(π/2) ├\n",
       "     └──┬───┬──┘└────┘└─────────┘\n",
       "q_1: ───┤ X ├────────────────────\n",
       "        └───┘                    \n",
       "c: 2/════════════════════════════\n",
       "                                 </pre>"
      ],
      "text/plain": [
       "global phase: π/4\n",
       "     ┌─────────┐┌────┐┌─────────┐\n",
       "q_0: ┤ Rz(π/2) ├┤ √X ├┤ Rz(π/2) ├\n",
       "     └──┬───┬──┘└────┘└─────────┘\n",
       "q_1: ───┤ X ├────────────────────\n",
       "        └───┘                    \n",
       "c: 2/════════════════════════════\n",
       "                                 "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circ_test.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">global phase: π/4\n",
       "     ┌─────────┐┌────┐┌─────────┐\n",
       "q_0: ┤ Rz(π/2) ├┤ √X ├┤ Rz(π/2) ├\n",
       "     └──┬───┬──┘└────┘└─────────┘\n",
       "q_1: ───┤ X ├────────────────────\n",
       "        └───┘                    </pre>"
      ],
      "text/plain": [
       "global phase: π/4\n",
       "     ┌─────────┐┌────┐┌─────────┐\n",
       "q_0: ┤ Rz(π/2) ├┤ √X ├┤ Rz(π/2) ├\n",
       "     └──┬───┬──┘└────┘└─────────┘\n",
       "q_1: ───┤ X ├────────────────────\n",
       "        └───┘                    "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm = generate_preset_pass_manager(backend=backend, optimization_level=1)\n",
    "circ_test_2 = pm.run(qc_2)\n",
    "\n",
    "circ_test_2.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11621/457931810.py:7: DeprecationWarning: Circuits that do not match the target hardware definition will no longer be supported after March 1, 2024. See the transpilation documentation (https://docs.quantum.ibm.com/transpile) for instructions to transform circuits and the primitive examples (https://docs.quantum.ibm.com/run/primitives-examples) to see this coupled with operator transformations.\n",
      "  job = estimator.run(circuits=[circ_test, circ_test], observables=observables)\n",
      "/home/jesshuan/miniconda3/envs/torch_quantum/lib/python3.12/site-packages/qiskit_ibm_runtime/qiskit_runtime_service.py:935: UserWarning: Cloud simulators have been deprecated and will be removed on 15 May 2024. Use the new local testing mode in qiskit-ibm-runtime version 0.22.0 or later to meet your debugging needs.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator results: EstimatorResult(values=array([-1.,  0.]), metadata=[{'variance': 0.0, 'shots': 32}, {'variance': 1.0, 'shots': 32}])\n"
     ]
    }
   ],
   "source": [
    "observables = [\"ZI\", \"IZ\"]\n",
    "\n",
    "\n",
    "with Session(service=service, backend=backend) as session:\n",
    "\n",
    "    estimator = Estimator(session=session, options=options)\n",
    "    job = estimator.run(circuits=[circ_test, circ_test], observables=observables)\n",
    "    print(f\"Estimator results: {job.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11621/1729889922.py:7: DeprecationWarning: Circuits that do not match the target hardware definition will no longer be supported after March 1, 2024. See the transpilation documentation (https://docs.quantum.ibm.com/transpile) for instructions to transform circuits and the primitive examples (https://docs.quantum.ibm.com/run/primitives-examples) to see this coupled with operator transformations.\n",
      "  job = estimator.run(circuits=[circ_test_2, circ_test_2], observables=observables)\n",
      "/home/jesshuan/miniconda3/envs/torch_quantum/lib/python3.12/site-packages/qiskit_ibm_runtime/qiskit_runtime_service.py:935: UserWarning: Cloud simulators have been deprecated and will be removed on 15 May 2024. Use the new local testing mode in qiskit-ibm-runtime version 0.22.0 or later to meet your debugging needs.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator results: EstimatorResult(values=array([-1.,  0.]), metadata=[{'variance': 0.0, 'shots': 32}, {'variance': 1.0, 'shots': 32}])\n"
     ]
    }
   ],
   "source": [
    "observables = [\"ZI\", \"IZ\"]\n",
    "\n",
    "\n",
    "with Session(service=service, backend=backend) as session:\n",
    "\n",
    "    estimator = Estimator(session=session, options=options)\n",
    "    job = estimator.run(circuits=[circ_test_2, circ_test_2], observables=observables)\n",
    "    print(f\"Estimator results: {job.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBase(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 mask=None,\n",
    "                 use_bias=False):\n",
    "        super(MultiHeadAttentionBase, self).__init__()\n",
    "\n",
    "        assert embed_dim % num_heads == 0, f\"Embedding dimension ({embed_dim}) should be divisible by number of heads ({num_heads})\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embed_dim // num_heads  # projection dimensions\n",
    "        self.k_linear = None\n",
    "        self.q_linear = None\n",
    "        self.v_linear = None\n",
    "        self.combine_heads = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn_weights = None\n",
    "    \n",
    "    def separate_heads(self, x):\n",
    "        '''\n",
    "        split into N heads\n",
    "        from (batch_size, seq_len, embed_dim)\n",
    "        to   (batch_size, seq_len, num_heads, embed_dim)\n",
    "        then transpose (1,2) to (batch_size, num_heads, seq_len, embed_dim)\n",
    "        to make mat mult straightforward for each head\n",
    "        '''\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    def attention(self, query, key, value, mask=None, dropout=None):\n",
    "        '''\n",
    "        Attention(Q, K, V) = softmax(Q K^T / sqrt(d_k))V\n",
    "        '''\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        # see also: https://tensorchiefs.github.io/dlday2018/tutorial/einsum.html\n",
    "        #scores = torch.einsum('bijh, bkjh -> bikh', query, key) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "        attn = torch.matmul(scores, value)\n",
    "        return attn, scores\n",
    "    \n",
    "    def downstream(self, query, key, value, batch_size, mask=None):\n",
    "        Q = self.separate_heads(query)\n",
    "        K = self.separate_heads(key)\n",
    "        V = self.separate_heads(value)\n",
    "\n",
    "        x, self.attn_weights = self.attention(Q, K, V, mask, dropout=self.dropout)\n",
    "\n",
    "        concat = x.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
    "\n",
    "        return concat\n",
    "        # output = self.combine_heads(concat)\n",
    "        # return output\n",
    "\n",
    "   # def forward(self, x, mask=None):\n",
    "    #    raise NotImplementedError(\"Base class does not execute forward function.\")\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionClassical(MultiHeadAttentionBase):\n",
    "    \n",
    "    def __init__(self, embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 dropout=0.1,\n",
    "                 mask=None,\n",
    "                 use_bias=False):\n",
    "        super(MultiHeadAttentionClassical, self).__init__(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, mask=mask, use_bias=use_bias)\n",
    "\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
    "        self.combine_heads = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        assert embed_dim == self.embed_dim, f\"Input embedding ({embed_dim}) does not match layer embedding size ({self.embed_dim})\"\n",
    "\n",
    "        K = self.k_linear(x)\n",
    "        Q = self.q_linear(x)\n",
    "        V = self.v_linear(x)\n",
    "\n",
    "        x = self.downstream(Q, K, V, batch_size, mask)\n",
    "        output = self.combine_heads(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLayer(tq.QuantumModule):\n",
    "        def __init__(self, n_qbits, *args, **kwargs):\n",
    "            super().__init__()    \n",
    "            self.n_wires = n_qbits\n",
    "            self.encoder = tq.GeneralEncoder(\n",
    "                    [{'input_idx': [i], 'func': 'rx', 'wires': [i]} for i in range(self.n_wires)])\n",
    "            #self.rx_list = [tq.RX(has_params=True, trainable=True) for _ in range(self.n_wires)]\n",
    "            #self.ry_test = tq.RY(has_params=True, trainable=True)\n",
    "            #self.measure = tq.MeasureAll(tq.PauliZ)\n",
    "            if n_qbits >= 2:\n",
    "                self.rx_0 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_1 = tq.RX(has_params=True, trainable=True)\n",
    "            if n_qbits >= 4:\n",
    "                self.rx_2 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_3 = tq.RX(has_params=True, trainable=True)\n",
    "            if n_qbits >= 8:\n",
    "                self.rx_4 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_5 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_6 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_7 = tq.RX(has_params=True, trainable=True)\n",
    "            if n_qbits >= 16:\n",
    "                self.rx_8 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_9 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_10 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_11 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_12 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_13 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_14 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_15 = tq.RX(has_params=True, trainable=True)\n",
    "            if n_qbits >= 32:\n",
    "                self.rx_16 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_17 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_18 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_19 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_20 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_21 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_22 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_23 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_24 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_25 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_26 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_27 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_28 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_29 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_30 = tq.RX(has_params=True, trainable=True)\n",
    "                self.rx_31 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "            #self.observables = SparsePauliOp.from_list([(\"ZX\" + \"I\"*(n_qbits - 2),1), (\"XY\" + \"I\"*(n_qbits - 2),1)] + [(\"I\"*i + \"Z\" + \"I\"*(n_qbits - 1 -i), 1) for i in range(n_qbits)])\n",
    "            #self.measure = tq.MeasureAll(tq.PauliZ)\n",
    "            #self.measure = tq.MeasureMultiPauliSum(self.observables)\n",
    "\n",
    "        def ansatz_gate_forward(self, q_device):\n",
    "            if self.n_wires >= 2:\n",
    "                self.rx_0(q_device, wires=0)\n",
    "                self.rx_1(q_device, wires=1)\n",
    "            if self.n_wires >= 4:\n",
    "                self.rx_2(q_device, wires=2)\n",
    "                self.rx_3(q_device, wires=3)\n",
    "            if self.n_wires >= 8:\n",
    "                self.rx_4(q_device, wires=4)\n",
    "                self.rx_5(q_device, wires=5)\n",
    "                self.rx_6(q_device, wires=6)\n",
    "                self.rx_7(q_device, wires=7)\n",
    "            if self.n_wires >= 16:\n",
    "                self.rx_8(q_device, wires=8)\n",
    "                self.rx_9(q_device, wires=9)\n",
    "                self.rx_10(q_device, wires=10)\n",
    "                self.rx_11(q_device, wires=11)\n",
    "                self.rx_12(q_device, wires=12)\n",
    "                self.rx_13(q_device, wires=13)\n",
    "                self.rx_14(q_device, wires=14)\n",
    "                self.rx_15(q_device, wires=15)\n",
    "            if self.n_wires >= 32:\n",
    "                self.rx_16(q_device, wires=16)\n",
    "                self.rx_17(q_device, wires=17)\n",
    "                self.rx_18(q_device, wires=18)\n",
    "                self.rx_19(q_device, wires=19)\n",
    "                self.rx_20(q_device, wires=20)\n",
    "                self.rx_21(q_device, wires=21)\n",
    "                self.rx_22(q_device, wires=22)\n",
    "                self.rx_23(q_device, wires=23)\n",
    "                self.rx_24(q_device, wires=24)\n",
    "                self.rx_25(q_device, wires=25)\n",
    "                self.rx_26(q_device, wires=26)\n",
    "                self.rx_27(q_device, wires=27)\n",
    "                self.rx_28(q_device, wires=26)\n",
    "                self.rx_29(q_device, wires=29)\n",
    "                self.rx_30(q_device, wires=30)\n",
    "                self.rx_31(q_device, wires=31)\n",
    "        \n",
    "        @tq.static_support\n",
    "        def forward(self, q_device, x, return_q_device=False):\n",
    "            self.encoder(q_device, x)\n",
    "            #for k in range(self.n_wires):\n",
    "                 #self.rx_list[k](q_device, wires=k)\n",
    "            #self.ry_test(q_device, wires=0)\n",
    "            self.ansatz_gate_forward(q_device)\n",
    "\n",
    "            for k in range(self.n_wires):\n",
    "                if k==self.n_wires-1:\n",
    "                    tqf.cnot(q_device, wires=[k, 0], static=self.static_mode, parent_graph=self.graph) \n",
    "                else:\n",
    "                    tqf.cnot(q_device, wires=[k, k+1], static=self.static_mode, parent_graph=self.graph)\n",
    "            q_device = q_device.bfloat16()\n",
    "            \n",
    "            if return_q_device:\n",
    "                return q_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = [\"I\"*i + \"ZX\"+ \"I\"*(4 - 2 - i) for i in range(4 - 1)] + [\"XX\" + \"I\"*(4-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ZXII', 'IZXI', 'IIZX', 'XXII']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IIXZ', 'IXZI', 'XZII', 'IIXX']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"\".join(reversed(obss)) for obss in obs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionQuantum(MultiHeadAttentionBase):\n",
    "    \n",
    "            \n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 dropout=0.1,\n",
    "                 mask=None,\n",
    "                 use_bias=False,\n",
    "                 n_qubits: int = 4,\n",
    "                 n_qlayers: int = 1,\n",
    "                 q_device=\"default.qubit\",\n",
    "                 session = None):\n",
    "        super(MultiHeadAttentionQuantum, self).__init__(embed_dim, num_heads, dropout=dropout, mask=mask, use_bias=use_bias)\n",
    "        \n",
    "        # todo: add intermediate layer to \"dress\" quantum circuit\n",
    "        assert n_qubits == embed_dim, \"Number of qubits ({n_qubits}) does not match embedding dim ({embed_dim})\"\n",
    "        self.n_qubits = n_qubits\n",
    "        #self.n_qlayers = n_qlayers\n",
    "\n",
    "        self.k_observables = [\"Z\"*(n_qubits - 1)]\n",
    "        self.q_observables = [\"Z\"*(n_qubits - 1)]\n",
    "        self.v_observables = [\"I\"*i + \"ZX\"+ \"I\"*(n_qubits - 2 - i) for i in range(n_qubits - 1)] + [\"XX\" + \"I\"*(n_qubits - 2)]\n",
    "\n",
    "        self.v_observables_reversed = [\"\".join(reversed(obs)) for obs in self.v_observables]\n",
    "\n",
    "        self.k_layer = QLayer(n_qubits)\n",
    "        self.q_layer = QLayer(n_qubits)\n",
    "        self.v_layer = QLayer(n_qubits)\n",
    "        #self.measure = tq.MeasureAll(tq.PauliZ)\n",
    "        self.q_device = q_device\n",
    "        self.session = session\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        assert embed_dim == self.embed_dim, f\"Input embedding ({embed_dim}) does not match layer embedding size ({self.embed_dim})\"\n",
    "\n",
    "        q_dev = tq.QuantumDevice(n_wires=self.n_qubits, device=self.q_device, bsz=x.shape[0])\n",
    "\n",
    "        options = Options(optimization_level=1, execution={})\n",
    "\n",
    "        if self.session is not None:\n",
    "            options = Options(optimization_level=1, execution={\"shots\":1024})\n",
    "            estimator = Estimator(session=self.session, options=options)\n",
    "            print(self.v_observables)\n",
    "            V = [estimator.run(circuits=[tq2qiskit(q_device=q_dev, m=self.v_layer, x=x[:, t, :].clone()) for o in range(len(self.v_observables))],\n",
    "                                     observables=self.v_observables).result() for t in range(seq_len)]\n",
    "            print(V)\n",
    "        else:\n",
    "            print(self.v_observables)\n",
    "            print(self.v_observables_reversed)\n",
    "            print([expval_joint_sampling_grouping(qdev=self.v_layer(q_dev, x[:, t, :].clone(), return_q_device=True), observables=self.v_observables_reversed, n_shots_per_group=1024) for t in range(seq_len)])\n",
    "            #print(torch.stack([torch.stack(list(expval_joint_sampling_grouping(qdev=self.v_layer(q_dev, x[:, t, :].clone(), return_q_device=True), observables=self.v_observables, n_shots_per_group=1024).values())) for t in range(seq_len)]))\n",
    "            #V = [self.v_layer(q_dev, x[:, t, :].clone()).values for t in range(seq_len)]\n",
    "        \n",
    "        #print(V)\n",
    "        \n",
    "        #K = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\n",
    "        #Q = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\n",
    "        #V = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\n",
    "\n",
    "        K = torch.Tensor(pad_sequence(K))\n",
    "        Q = torch.Tensor(pad_sequence(Q))\n",
    "        V = torch.Tensor(pad_sequence(V))\n",
    "        x = self.downstream(Q, K, V, batch_size, mask)\n",
    "        #output = [self.q_layer(x[:, t, :],q_dev) for t in range(seq_len)]\n",
    "        #output = torch.Tensor(pad_sequence(output)).clone()\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 4\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "SEQ_LEN = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "classical_module = MultiHeadAttentionClassical(embed_dim=EMBED_DIM, num_heads=4, dropout=0.0)\n",
    "#quantum_module = MultiHeadAttentionQuantum(embed_dim=EMBED_DIM, num_heads=4, dropout=0.0, n_qubits=EMBED_DIM, q_device=\"cuda\", session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.tensor(np.random.rand(BATCH_SIZE, SEQ_LEN, EMBED_DIM), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = classical_module(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 4])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantum_module = MultiHeadAttentionQuantum(embed_dim=EMBED_DIM, num_heads=4, dropout=0.0, n_qubits=EMBED_DIM, q_device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ZXII', 'IZXI', 'IIZX', 'XXII']\n",
      "['IIXZ', 'IXZI', 'XZII', 'IIXX']\n",
      "[{'IXZI': tensor([-0.0391]), 'IIXZ': tensor([-0.0664]), 'XZII': tensor([-0.2559]), 'IIXX': tensor([-0.0137])}]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'K' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb Cell 44\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m output_q \u001b[39m=\u001b[39m quantum_module(test_input)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_quantum/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_quantum/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb Cell 44\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X42sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39mprint\u001b[39m([expval_joint_sampling_grouping(qdev\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_layer(q_dev, x[:, t, :]\u001b[39m.\u001b[39mclone(), return_q_device\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), observables\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_observables_reversed, n_shots_per_group\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(seq_len)])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X42sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39m#print(torch.stack([torch.stack(list(expval_joint_sampling_grouping(qdev=self.v_layer(q_dev, x[:, t, :].clone(), return_q_device=True), observables=self.v_observables, n_shots_per_group=1024).values())) for t in range(seq_len)]))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X42sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39m#V = [self.v_layer(q_dev, x[:, t, :].clone()).values for t in range(seq_len)]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X42sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X42sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m#Q = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X42sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m#V = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X42sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m K \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(pad_sequence(K))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X42sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m Q \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(pad_sequence(Q))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X42sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m V \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(pad_sequence(V))\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'K' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "output_q = quantum_module(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ZXII', 'IZXI', 'IIZX', 'XXII']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11621/4187400498.py:46: DeprecationWarning: Circuits that do not match the target hardware definition will no longer be supported after March 1, 2024. See the transpilation documentation (https://docs.quantum.ibm.com/transpile) for instructions to transform circuits and the primitive examples (https://docs.quantum.ibm.com/run/primitives-examples) to see this coupled with operator transformations.\n",
      "  V = [estimator.run(circuits=[tq2qiskit(q_device=q_dev, m=self.v_layer, x=x[:, t, :].clone()) for o in range(len(self.v_observables))],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EstimatorResult(values=array([-0.265625  ,  0.41992188, -0.47851562,  0.03125   ]), metadata=[{'variance': 0.929443359375, 'shots': 1024}, {'variance': 0.8236656188964844, 'shots': 1024}, {'variance': 0.7710227966308594, 'shots': 1024}, {'variance': 0.9990234375, 'shots': 1024}])]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'K' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb Cell 45\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m Session(backend\u001b[39m=\u001b[39mbackend) \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     quantum_module \u001b[39m=\u001b[39m MultiHeadAttentionQuantum(embed_dim\u001b[39m=\u001b[39mEMBED_DIM, num_heads\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, dropout\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, n_qubits\u001b[39m=\u001b[39mEMBED_DIM, q_device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, session\u001b[39m=\u001b[39msession)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     output_q \u001b[39m=\u001b[39m quantum_module(test_input)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_quantum/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_quantum/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb Cell 45\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X43sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39mprint\u001b[39m([expval_joint_sampling_grouping(qdev\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_layer(q_dev, x[:, t, :]\u001b[39m.\u001b[39mclone(), return_q_device\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), observables\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_observables_reversed, n_shots_per_group\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(seq_len)])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X43sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39m#print(torch.stack([torch.stack(list(expval_joint_sampling_grouping(qdev=self.v_layer(q_dev, x[:, t, :].clone(), return_q_device=True), observables=self.v_observables, n_shots_per_group=1024).values())) for t in range(seq_len)]))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X43sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39m#V = [self.v_layer(q_dev, x[:, t, :].clone()).values for t in range(seq_len)]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X43sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X43sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m#Q = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X43sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m#V = [self.q_layer(x[:, t, :].clone(),q_dev) for t in range(seq_len)]\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X43sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m K \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(pad_sequence(K))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X43sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m Q \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(pad_sequence(Q))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesshuan/Desktop/RETNET/Quantum_Retnet_project/example_quauntum_transfomer_on_real.ipynb#X43sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m V \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(pad_sequence(V))\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'K' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "with Session(backend=backend) as session:\n",
    "    quantum_module = MultiHeadAttentionQuantum(embed_dim=EMBED_DIM, num_heads=4, dropout=0.0, n_qubits=EMBED_DIM, q_device=\"cpu\", session=session)\n",
    "    output_q = quantum_module(test_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retnet_experiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
